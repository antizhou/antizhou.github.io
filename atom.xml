<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>World of Anti Zhou</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.antizhou.com/"/>
  <updated>2019-05-21T12:57:14.985Z</updated>
  <id>http://www.antizhou.com/</id>
  
  <author>
    <name>Anti Zhou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://www.antizhou.com/db/hbase/hbase/"/>
    <id>http://www.antizhou.com/db/hbase/hbase/</id>
    <published>2019-05-21T12:57:14.985Z</published>
    <updated>2019-05-21T12:57:14.985Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么不建议在-HBase-中使用过多的列族"><a href="#为什么不建议在-HBase-中使用过多的列族" class="headerlink" title="为什么不建议在 HBase 中使用过多的列族"></a>为什么不建议在 HBase 中使用过多的列族</h2><p>内存<br>磁盘</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;为什么不建议在-HBase-中使用过多的列族&quot;&gt;&lt;a href=&quot;#为什么不建议在-HBase-中使用过多的列族&quot; class=&quot;headerlink&quot; title=&quot;为什么不建议在 HBase 中使用过多的列族&quot;&gt;&lt;/a&gt;为什么不建议在 HBase 中使用过多的
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>时序数据库对比</title>
    <link href="http://www.antizhou.com/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%B9%E6%AF%94/"/>
    <id>http://www.antizhou.com/时序数据库对比/</id>
    <published>2019-05-19T06:10:51.000Z</published>
    <updated>2019-05-19T06:40:30.157Z</updated>
    
    <content type="html"><![CDATA[<p>时序数据库技术体系中一个非常重要的技术点是时序数据模型设计，不同的时序系统有不同的设计模式，而不同的设计模式对时序数据的读写性能、数据压缩效率等各个方面都有非常重要的影响。这篇文章笔者将会分别针对OpenTSDB、Druid、InfluxDB以及Beringei这四个时序系统中的时序数据模型设计进行介绍。</p><p>在详细介绍时序数据模型之前，还是有必要简单回顾一下时序数据的几个基本概念，如下图所示：<br><img src="https://raw.githubusercontent.com/antizhou/material/master/images/v2-b734d776ef3c1dcd62788c6098c31ee9_hd.jpg" alt="image"></p><p>上图是一个典型的时序数据示意图，由图中可以看出，时序数据由两个维度坐标来表示，横坐标表示时间轴，随着时间的不断流逝，数据也会源源不断地吐出来；和横坐标不同，纵坐标由两种元素构成，分别是数据源和metric，数据源由一系列的标签（tag，也称为维度）唯一表示，图中数据源是一个广告数据源，这个数据源由publisher、advertiser、gender以及country四个维度值唯一表示，metric表示待收集的数据源指标。一个数据源通常会采集很多指标（metric），上图中广告数据源就采集了impressions、clicks以及revenue这三种指标，分别表示广告浏览量、广告点击率以及广告收入。</p><p>看到这里，相信大家对时序数据已经有了一个初步的了解，可以简单的概括为：一个时序数据点（point）由datasource(tags)+metric+timestamp这三部分唯一确定。然而，这只是逻辑上的概念理解，那具体的时序数据库到底是如何将这样一系列时序数据点进行存储的呢？下文笔者针对OpenTSDB、Druid、InfluxDB以及Beringei四种系统进行介绍。</p><h2 id="OpenTSDB-HBase-时序数据存储模型"><a href="#OpenTSDB-HBase-时序数据存储模型" class="headerlink" title="OpenTSDB(HBase)时序数据存储模型"></a>OpenTSDB(HBase)时序数据存储模型</h2><p>OpenTSDB基于HBase存储时序数据，在HBase层面设计RowKey规则为：metric+timestamp+datasource(tags)。HBase是一个KV数据库，一个时序数据(point)如果以KV的形式表示，那么其中的V必然是point的具体数值，而K就自然而然是唯一确定point数值的datasource+metric+timestamp。这种规律不仅适用于HBase，还适用于其他KV数据库，比如Kudu。</p><p>既然HBase中K是由datasource、metric以及timestamp三者构成，现在我们可以简单认为rowkey就为这三者的组合，那问题来了：这三者的组合顺序是怎么样的呢？</p><p>首先来看哪个应该排在首位。因为HBase中一张表的数据组织方式是按照rowkey的字典序顺序排列的，为了将同一种指标的所有数据集中放在一起，HBase将将metric放在了rowkey的最前面。假如将timestamp放在最前面，同一时刻的数据必然会写入同一个数据分片，无法起到散列的效果；而如果将datasource（即tags）放在最前面的话，这里有个更大的问题，就是datasource本身由多个标签组成，如果用户指定其中部分标签查找，而且不是前缀标签的话，在HBase里面将会变成大范围的扫描过滤查询，查询效率非常之低。举个上面的例子，如果将datasource放在最前面，那rowkey就可以表示为publisher=ultrarimfast.com&amp;advertiser:google.com&amp;gender:Male&amp;country:USA_impressions_20110101000000，此时用户想查找20110101000000这个时间点所有发布在USA的所有广告的浏览量，即只根据country=USA这样一个维度信息查找指定时间点的某个指标，而且这个维度不是前缀维度，就会扫描大量的记录进行过滤。</p><p>确定了metric放在最前面之后，再来看看接下来应该将datasource放在中间呢还是应该将timestamp放在中间？将metric放在前面已经可以解决请求均匀分布（散列）的要求，因此HBase将timestamp放在中间，将datasource放在最后。试想，如果将datasource放在中间，也会遇到上文中说到的后缀维度查找的问题。</p><p>因此，OpenTSDB中rowkey的设计为：metric+timestamp+datasource，好了，那HBase就可以只设置一个columnfamily和一个column。那问题来了，OpenTSDB的这种设计有什么问题？在了解设计问题之前需要简单看看HBase在文件中存储KV的方式，即一系列时序数据在文件、内存中的存储方式，如下图所示：</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/v2-f5949de041cae2639dd7db6d1ca8dc29_hd.jpg" alt="image"></p><p>上图是HBase中一个存储KeyValue(KV)数据的数据块结构，一个数据块由多个KeyValue数据组成，在我们的事例中KeyValue就是一个时序数据点（point）。其中Value结构很简单，就是一个数值。而Key就比较复杂了，由rowkey+columnfamily+column+timestamp+keytype组成，其中rowkey等于metric+timestamp+datasource。</p><p>问题一：存在很多无用的字段。一个KeyValue中只有rowkey是有用的，其他字段诸如columnfamily、column、timestamp以及keytype从理论上来讲都没有任何实际意义，但在HBase的存储体系里都必须存在，因而耗费了很大的存储成本。</p><p>问题二：数据源和采集指标冗余。KeyValue中rowkey等于metric+timestamp+datasource，试想同一个数据源的同一个采集指标，随着时间的流逝不断吐出采集数据，这些数据理论上共用同一个数据源(datasource)和采集指标(metric)，但在HBase的这套存储体系下，共用是无法体现的，因此存在大量的数据冗余，主要是数据源冗余以及采集指标冗余。</p><p>问题三：无法有效的压缩。HBase提供了块级别的压缩算法－snappy、gzip等，这些通用压缩算法并没有针对时序数据进行设置，压缩效率比较低。HBase同样提供了一些编码算法，比如FastDiff等等，可以起到一定的压缩效果，但是效果并不佳。效果不佳的主要原因是HBase没有数据类型的概念，没有schema的概念，不能针对特定数据类型进行特定编码，只能选择通用的编码，效果可想而知。</p><p>问题四：不能完全保证多维查询能力。HBase本身没有schema，目前没有实现倒排索引机制，所有查询必须指定metric、timestamp以及完整的tags或者前缀tags进行查询，对于后缀维度查询也勉为其难。</p><p>虽说有这样那样的问题，但是OpenTSDB还是针对存储模型做了两个方面的优化：</p><p>优化一：timestamp并不是想象中细粒度到秒级或毫秒级，而是精确到小时级别，然后将小时中每一秒设置到列上。这样一行就会有3600列，每一列表示一小时的一秒。这样设置据说可以有效的取出一小时整的数据。</p><p>优化二：所有metrics以及所有标签信息（tags）都使用了全局编码将标签值编码成更短的bit，减少rowkey的存储数据量。上文分析HBase这种存储方式的弊端是说道会存在大量的数据源(tags)冗余以及指标(metric)冗余，有冗余是吧，那我就搞个编码，将string编码成bit，尽最大努力减少冗余。虽说这样的全局编码可以有效降低数据的存储量，但是因为全局编码字典需要存储在内存中，因此在很多时候（海量标签值），字典所需内存都会非常之大。</p><p>上述两个优化可以参考OpenTSDB这张经典的示意图：<br><img src="https://raw.githubusercontent.com/antizhou/material/master/images/v2-1e76d8c875d96b6f7b1331997542c44f_hd.jpg" alt="image"></p><h2 id="Druid时序数据存储模型设计"><a href="#Druid时序数据存储模型设计" class="headerlink" title="Druid时序数据存储模型设计"></a>Druid时序数据存储模型设计</h2><p>和HBase和Kudu这类KV数据库不同，Druid是另一种玩法。Druid是一个不折不扣的列式存储系统，没有HBase的主键。上述时序数据在Druid中表示是下面这个样子的：<br><img src="https://raw.githubusercontent.com/antizhou/material/master/images/v2-4d2485ed6aeed4d6edff95c6c313e438_r.jpg" alt="image"></p><p>Druid是一个列式数据库，所以每一列都会独立存储，比如Timestamp列会存储在一起形成一个文件，publish列会存储在一起形成一个文件，以此类推。细心的童鞋就会说了，这样存储，依然会有数据源（tags）大量冗余的问题。针对冗余这个问题，Druid和HBase的处理方式一样，都是采用编码字典对标签值进行编码，将string类型的标签值编码成int值。但和HBase不一样的是，Druid编码是局部编码，Druid和HBase都采用LSM结构，数据先写入内存再flush到数据文件，Druid编码是文件级别的，局部编码可以有效减小对内存的巨大压力。除此之外，Druid的这种列式存储模式还有如下好处：</p><ol><li>数据存储压缩率高。每列独立存储，可以针对每列进行压缩，而且可以为每列设置对应的压缩策略，比如时间列、int、fload、double、string都可以分别进行压缩，压缩效果更好。</li><li>支持多维查找。Druid为datasource的每个列分别设置了Bitmap索引，利用Bitmap索引可以有效实现多维查找，比如用户想查找20110101T00:00:00这个时间点所有发布在USA的所有广告的浏览量，可以根据country=USA在Bitmap索引中找到要找的行号，再根据行号定位待查的metrics。</li></ol><p>然而，这样的存储模型也有一些问题：</p><ol><li>数据依然存在冗余。和OpenTSDB一样，tags存在大量的冗余。</li><li>指定数据源的范围查找并没有OpenTSDB高效。这是因为Druid会将数据源拆开成多个标签，每个标签都走Bitmap索引，再最后使用与操作找到满足条件的行号，这个过程需要一定的开销。而OpenTSDB中直接可以根据数据源拼成rowkey，查找走B+树索引，效率必然会更高。</li></ol><h2 id="InfluxDB时序数据存储模型设计"><a href="#InfluxDB时序数据存储模型设计" class="headerlink" title="InfluxDB时序数据存储模型设计"></a>InfluxDB时序数据存储模型设计</h2><p>相比OpenTSDB以及Druid，可能很多童鞋对InfluxDB并不特别熟悉，然而在时序数据库排行榜单上InfluxDB却是遥遥领先。InfluxDB是一款专业的时序数据库，只存储时序数据，因此在数据模型的存储上可以针对时序数据做非常多的优化工作。</p><p>为了保证写入的高效，InfluxDB也采用LSM结构，数据先写入内存，当内存容量达到一定阈值之后flush到文件。InfluxDB在时序数据模型设计方面提出了一个非常重要的概念：seriesKey，seriesKey实际上就是measurement+datasource(tags)。需要特别注意的是，measurement和上文中提到的measurement并不是一回事，上文中measurement和metric意义相同，表示采集指标，而InfluxDB中measurement更像是表的概念，InfluxDB中使用fields表示指标，如下图所示：<br><img src="https://raw.githubusercontent.com/antizhou/material/master/images/td9.png" alt="image"></p><p>时序数据写入内存之后按照seriesKey进行组织：<br><img src="https://raw.githubusercontent.com/antizhou/material/master/images/td10.png" alt="image"></p><p>内存中实际上就是一个Map：&lt;SeriesKey+fieldKey, List&lt;Timestamp|Value&gt;&gt;，Map中一个SeriesKey+fieldKey对应一个List，List中存储时间线数据。数据进来之后根据measurement+datasource(tags)拼成SeriesKey，加上fieldKey，再将Timestamp|Value组合值写入时间线数据List中。内存中的数据flush的文件后，同样会将同一个SeriesKey中的时间线数据写入同一个Block块内，即一个Block块内的数据都属于同一个数据源下的同一个field。</p><p>这种设计我们认为是将时间序列数据按照时间线挑了出来。先来看看这样设计的好处：</p><p>好处一：同一数据源的tags不再冗余存储。一个Block内的数据都共用一个SeriesKey，只需要将这个SeriesKey写入这个Block的Trailer部分就可以。大大降低了时序数据的存储量。</p><p>好处二：时间序列和value可以在同一个Block内分开独立存储，独立存储就可以对时间列以及数值列分别进行压缩。InfluxDB对时间列的存储借鉴了Beringei的压缩方式，使用delta-delta压缩方式极大的提高了压缩效率。而对Value的压缩可以针对不同的数据类型采用相同的压缩效率。</p><p>好处三：对于给定数据源以及时间范围的数据查找，可以非常高效的进行查找。这一点和OpenTSDB一样。</p><p>细心的同学可能会问了，将datasource(tags)和metric拼成SeriesKey，不是也不能实现多维查找。确实是这样，不过InfluxDB内部实现了倒排索引机制，即实现了tag到SeriesKey的映射关系，如果用户想根据某个tag查找的话，首先根据tag在倒排索引中找到对应的SeriesKey，再根据SeriesKey定位具体的时间线数据。InfluxDB的这种存储引擎称为TSM，全称为Timestamp-Structure Merge Tree，基本原理类似于LSM。后期笔者将会对InfluxDB的数据写入、文件格式、倒排索引以及数据读取进行专题介绍。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.zhihu.com/question/50194483" target="_blank" rel="noopener">https://www.zhihu.com/question/50194483</a><br><a href="http://hbasefly.com/2017/11/19/timeseries-database-2/?bcvkrm=rgmdr1" target="_blank" rel="noopener">http://hbasefly.com/2017/11/19/timeseries-database-2/?bcvkrm=rgmdr1</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;时序数据库技术体系中一个非常重要的技术点是时序数据模型设计，不同的时序系统有不同的设计模式，而不同的设计模式对时序数据的读写性能、数据压缩效率等各个方面都有非常重要的影响。这篇文章笔者将会分别针对OpenTSDB、Druid、InfluxDB以及Beringei这四个时序系
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>限流策略</title>
    <link href="http://www.antizhou.com/hahc/%E9%99%90%E6%B5%81%E7%AD%96%E7%95%A5/"/>
    <id>http://www.antizhou.com/hahc/限流策略/</id>
    <published>2019-05-17T00:28:56.000Z</published>
    <updated>2019-05-19T04:35:33.191Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基于计数器的单机限流"><a href="#基于计数器的单机限流" class="headerlink" title="基于计数器的单机限流"></a>基于计数器的单机限流</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//限流计数器</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> AtomicLong counter = <span class="keyword">new</span> AtomicLong();</span><br><span class="line"><span class="comment">//限流阈值</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> counterMax = <span class="number">500</span>;</span><br><span class="line"><span class="comment">//业务处理方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Request request)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//请求过滤</span></span><br><span class="line">        <span class="keyword">if</span> (counter.incrementAndGet() &gt; counterMax) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//业务逻辑</span></span><br><span class="line">        doBusiness(request);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="comment">//错误处理</span></span><br><span class="line">        doException(request,e);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        counter.decrementAndGet();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>优点：代码简洁，操作方便<br>缺点：先到先得，先到的请求可执行概率为100%，后到的请求可执行概率小一些，每个请求获得执行的机会是不平等的。</p><h2 id="基于随机数的单机限流"><a href="#基于随机数的单机限流" class="headerlink" title="基于随机数的单机限流"></a>基于随机数的单机限流</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取随机数</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> ThreadLocalRandom ptgGenerator = ThreadLocalRandom.current();</span><br><span class="line"><span class="comment">//限流百分比，允许多少流量通过此业务，这里限定为10%</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> ptgGuarder = <span class="number">10</span>;</span><br><span class="line"><span class="comment">//业务处理方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Request request)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//请求进入，获取百分比</span></span><br><span class="line">        <span class="keyword">int</span> currentPercentage = ptgGenerator.nextInt(<span class="number">1</span>, <span class="number">100</span>);</span><br><span class="line">        <span class="keyword">if</span> (currentPercentage &lt;= ptgGuarder) &#123;</span><br><span class="line">            <span class="comment">//业务处理</span></span><br><span class="line">            doBusiness(request);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="comment">//错误处理</span></span><br><span class="line">        doException(request, e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>优点：代码简洁，操作简便，每个请求可执行的机会是平等的。<br>缺点：不适合应用突增的流量。</p><h2 id="基于固定、滑动时间窗口的单机限流"><a href="#基于固定、滑动时间窗口的单机限流" class="headerlink" title="基于固定、滑动时间窗口的单机限流"></a>基于固定、滑动时间窗口的单机限流</h2><p>问题：<br>1、粒度太大了，不均匀，针对1秒以下的，没法辨析。<br>我们能不能把粒度拆细了，1秒拆成10个100毫秒。每一个100毫秒有一个计数器。了解TCP/IP的应该知道，TCP/IP为了增加传输速度和控制传输速度，有个叫“滑动窗口协议”。<br>就算拆得再细，也无法解决匀速限制速度的问题。</p><p>2、临界点问题<br>而且还有个临界点问题，假如，一秒限制10个请求，在第1秒和第2秒之间，第1秒后半段时间10个请求，第2秒前半段10个请求，那第1秒后半段+第2秒前半段时间组成的一秒钟里就有20个请求，没有起到限速的作用。</p><h2 id="基于漏桶算法的单机限流"><a href="#基于漏桶算法的单机限流" class="headerlink" title="基于漏桶算法的单机限流"></a>基于漏桶算法的单机限流</h2><p>漏桶作为计量工具时，可以用于流量整型和流量控制，网上大多数关于漏桶算法的描述如下:</p><ul><li>一个固定容量的漏桶，按照常量固定速率流出水滴</li><li>如果桶是空的，则不需流出水滴</li><li>可以以任意速率流入水滴到漏桶</li><li>如果流入水滴超出了桶的容量，则流入的水滴溢出了(被丢弃)，而漏桶容量是不变的</li></ul><p>漏桶算法思路很简单，水(数据或者请求)先进入到漏桶里，漏桶以一定的速度出水，当水流入速度过大会直接溢出，可以看出漏桶算法能强行限制数据的传输速率。</p><blockquote><p>漏桶它的主要目的是控制注入到网络的速率，平滑网络上的突发流量。漏桶算法提供了一种机制，通过它，突发流量可以被整形以便为网络提供了一个稳定的流量。漏桶可以看做是一个带有常量服务时间的单服务器队列，如果漏桶(包缓存)溢出，那么数据包会被丢弃。</p></blockquote><p>在某些情况下，漏桶算法不能够有效使用网络资源，因为漏桶的漏出速率是固定的参数，所以，即使网络中不存在资源冲突(没有发生拥塞)，漏桶算法也不能使某一个单独的流突发到端口速率。因此，对于存在突发特性的流量来说缺乏效率，而令牌桶算法则能够满足这些具有突发特性的流量。</p><p>通常，漏桶算法与令牌桶算法可以结合起来为网络流量提供更大的控制</p><p>优缺点: </p><ol><li>漏桶算法优点很明显，简单、高效，能恰当拦截容量外的暴力流量。 </li><li>缺点也明显，无法对流量做频率处理。比如:桶大小设置范围内，进行并发攻击依然能产生大流量并发效果，桶容量又不可以设置的过小，否则容易卡死正常用户。</li></ol><h2 id="基于令牌桶算法的单机限流"><a href="#基于令牌桶算法的单机限流" class="headerlink" title="基于令牌桶算法的单机限流"></a>基于令牌桶算法的单机限流</h2><p>令牌桶算法的原理是系统会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。桶中最多存放一定数量的令牌,当桶满时，新添加的令牌被丢弃或拒绝</p><p>令牌桶的另一个好处是可以方便的改变速度，一旦需要提高速率，则按需提高放入桶中的令牌的速率。一般会定时(比如100毫秒)往桶中增加一定数量的令牌，有些变种的算法则实时的计算应该增加的令牌的数量。</p><p>优缺点: </p><ol><li>令牌桶的另外一个好处是可以方便的改变速度。 一旦需要提高速率，则按需提高放入桶中的令牌的速率。 </li><li>可以限制总请求大小，还限制平均频率大小； </li><li>还是容易导致误判等问题</li><li>对比基于时间窗口的限流算法，令牌桶和漏桶算法对流量整形效果比时间窗口算法要好很多，但是并不是整形效果越好就越合适，对于没有提前预热的令牌桶，如果做否决式限流，会导致误杀很多请求。上述算法中当 n 比较小时，比如 50，间隔 20ms 才会向桶中放入一个令牌，而接口的访问在 1s 内可能随机性很强，这就会出现：尽管从曲线上看对最大访问频率的限制很有效，流量在细时间粒度上面都很平滑，但是误杀了很多本不应该拒绝的接口请求。</li></ol><h2 id="令牌桶算法和漏桶算法比较"><a href="#令牌桶算法和漏桶算法比较" class="headerlink" title="令牌桶算法和漏桶算法比较"></a>令牌桶算法和漏桶算法比较</h2><p>令牌桶和漏桶比较:</p><ul><li>针对突发性流量<ul><li>令牌桶可以限制平均流入速率，只要有令牌，允许一定程度的突发流量</li><li>漏桶流入速率任意，流出速率常量，即平滑突发流入速率</li></ul></li></ul><p>它们之间最主要的差别在于：漏桶算法能够强行限制数据的传输速率，而令牌桶算法能够在限制数据的平均传输速率的同时还允许某种程度的突发传输。</p><p>在“令牌桶算法”中，只要令牌桶中存在令牌，那么就允许突发地传输数据直到达到用户配置的上限，因此它适合于具有突发特性的流量。</p><h2 id="基于redis-lua的分布式限流"><a href="#基于redis-lua的分布式限流" class="headerlink" title="基于redis lua的分布式限流"></a>基于redis lua的分布式限流</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">--限流的key</span><br><span class="line">local key = &apos;limitkey&apos;..KEYS[1]</span><br><span class="line"></span><br><span class="line">--累加请求数</span><br><span class="line">local val = tonumber(redis.call(&apos;get&apos;, key) or 0)</span><br><span class="line"></span><br><span class="line">--限流阈值</span><br><span class="line">local threshold = tonumber(ARGV[1])</span><br><span class="line"></span><br><span class="line">if  val&gt;threshold then</span><br><span class="line">    --请求被限</span><br><span class="line">    return 0</span><br><span class="line">else</span><br><span class="line">    --递增请求数</span><br><span class="line">    redis.call(&apos;INCRBY&apos;, key, &quot;1&quot;)</span><br><span class="line">    --5秒后过期</span><br><span class="line">    redis.call(&apos;expire&apos;, key, 5)</span><br><span class="line">    --请求通过</span><br><span class="line">    return 1</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>优点：集群整体流量控制，防止雪崩效应<br>缺点：需要引入额外的redis组件，且要求redis支持lua脚本。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://pjmike.github.io/2018/09/14/%E6%B5%85%E8%B0%88%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">浅谈限流算法</a><br><a href="https://maiyang.me/post/2017-05-28-rate-limit-algorithm/" target="_blank" rel="noopener">限流:漏桶算法和令牌桶算法</a><br><a href="https://mp.weixin.qq.com/s/9cDIMBx3neN2wz0LNxBeRA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/9cDIMBx3neN2wz0LNxBeRA</a><br><a href="https://juejin.im/entry/5be00d916fb9a049f153a70e" target="_blank" rel="noopener">https://juejin.im/entry/5be00d916fb9a049f153a70e</a><br><a href="https://zhuanlan.zhihu.com/p/47863288" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47863288</a><br><a href="https://www.infoq.cn/article/microservice-interface-rate-limit" target="_blank" rel="noopener">https://www.infoq.cn/article/microservice-interface-rate-limit</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基于计数器的单机限流&quot;&gt;&lt;a href=&quot;#基于计数器的单机限流&quot; class=&quot;headerlink&quot; title=&quot;基于计数器的单机限流&quot;&gt;&lt;/a&gt;基于计数器的单机限流&lt;/h2&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>redis的过期策略如何实现</title>
    <link href="http://www.antizhou.com/redis/redis%E7%9A%84%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0/"/>
    <id>http://www.antizhou.com/redis/redis的过期策略如何实现/</id>
    <published>2019-05-15T12:45:17.000Z</published>
    <updated>2019-05-15T12:49:07.983Z</updated>
    
    <content type="html"><![CDATA[<p>面试官：我看你简历提到xxx项目使用了redis</p><p>小弱鸡：嗯，因为xxxx的性能问题，经过排查之后，发现性能瓶颈在数据库上面，所以引入了redis</p><p>面试官：行，那你了解redis的过期策略吗？</p><p>小弱鸡：有了解过，因为redis是基于内存来进行高性能、高并发的读写操作的，既然是内存，那肯定有空间的限制，如果只有10g内存，一直往里面写数据，那肯定不行，所以采用一些过期策略把不需要的数据删除、或者是淘汰掉。</p><p>面试官：那都有哪些过期策略？</p><p>小弱鸡：我了解的有 定期删除、惰性删除两种</p><p>面试官：你先讲讲定期删除怎么实现？</p><p>小弱鸡好像有点兴奋：所谓定期删除，指的是 redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。</p><p>面试官：为什么是随机抽取？</p><p>小弱鸡：假如在redis 里插入10w个key，并且都设置了过期时间，如果每次都检查所有key，那cpu基本上都消耗在过期key的检查上了，redis对外的性能也会大大降低，简直就是一场灾难。</p><p>面试官：随机检查会存在什么问题？</p><p>小弱鸡：可能导致本已经过期的key没有被扫描到，而继续留在内存中，并占用空间，等待被删除。</p><p>面试官：这种情况怎么解决？</p><p>小弱鸡又兴奋了：这时候就需要第二种过期策略了，惰性删除，就是在获取某个 key 的时候，redis 会检查一下 ，如果这个 key 设置了过期时间，并且已经过期了，那么就直接删除，返回空。</p><p>面试官面带一丝笑意：嗯，那再考虑一种情况，如果大量的key没有被扫描到，且已过期，也没有被再次访问，即没有走惰性删除，这些大量过期 key 堆积在内存里，导致 redis 内存块耗尽了，这种情况下，怎么办？</p><p>小菜鸡想了会，抓了抓脑袋：redis内部提供了内存淘汰机制，应该有好几种策略，但我只知道LRU算法。</p><p>面试官：嗯，那你手写一个LRU算法？</p><p>小菜鸡*花一紧，这不是给自己挖坑么！！！ 如果从头开始写一个完整的LRU算法，那会要了命，幸好小菜鸡还记得 LinkedHashMap，可以基于 LinkedHashMap实现一个简单版本的LRU算法。</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/image-20190515204727553.png" alt="image-20190515204727553"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;面试官：我看你简历提到xxx项目使用了redis&lt;/p&gt;
&lt;p&gt;小弱鸡：嗯，因为xxxx的性能问题，经过排查之后，发现性能瓶颈在数据库上面，所以引入了redis&lt;/p&gt;
&lt;p&gt;面试官：行，那你了解redis的过期策略吗？&lt;/p&gt;
&lt;p&gt;小弱鸡：有了解过，因为redis是基于
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>MappedByteBuffer VS FileChannel 孰强孰弱？</title>
    <link href="http://www.antizhou.com/io/MappedByteBuffer%20VS%20FileChannel/"/>
    <id>http://www.antizhou.com/io/MappedByteBuffer VS FileChannel/</id>
    <published>2019-05-13T15:12:56.000Z</published>
    <updated>2019-05-13T15:20:59.304Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Java 在 JDK 1.4 引入了 ByteBuffer 等 NIO 相关的类，使得 Java 程序员可以抛弃基于 Stream ，从而使用基于 Block  的方式读写文件，另外，JDK 还引入了 IO 性能优化之王—— 零拷贝  sendFile 和 mmap。但他们的性能究竟怎么样？ 和 RandomAccessFile 比起来，快多少？ 什么情况下快？到底是 FileChannel 快还是 MappedByteBuffer 快……</p><p>(零拷贝参考 <a href="https://www.linuxjournal.com/article/6345" target="_blank" rel="noopener">Zero Copy I: User-Mode Perspective</a>)</p><p>天啊，问题太多了！！！！！！</p><p>让我们慢慢分析。</p><h2 id="看看善于利用-IO-零拷贝的-MQ-们"><a href="#看看善于利用-IO-零拷贝的-MQ-们" class="headerlink" title="看看善于利用 IO 零拷贝的 MQ 们"></a>看看善于利用 IO 零拷贝的 MQ 们</h2><p>我们知道，Java 世界有很多 MQ：ActiveMQ，kafka，RocketMQ，去哪儿 MQ，而他们则是 Java 世界使用 NIO 零拷贝的大户。</p><p>然而，他们的性能却大相同，抛开其他的因素，例如网络传输方式，数据结构设计，文件存储方式，我们仅仅讨论 Broker 端对文件的读写，看看他们有什么不同。</p><p>下图是楼主查看源码总结的各个 MQ 使用的文件读写方式。</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/1557669126362-bb705e6f-3289-45ff-9f51-38debe36c3e3.png" alt="img"></p><ul><li>kafka：record 的读写都是基于 FileChannel。index 读写基于 MMAP（厮大提示）。</li><li>RocketMQ：读盘基于 MMAP，写盘默认使用 MMAP，可通过修改配置，配置成 FileChannel，原因是作者想避免 PageCache 的锁竞争，通过两层架构实现读写分离。</li><li>QMQ： 去哪儿 MQ，读盘使用 MMAP，写盘使用 FileChannel。</li><li>ActiveMQ 5.15： 读写全部都是基于 RandomAccessFile，这也是我们抛弃 ActiveMQ 的原因。</li></ul><p>那么，到底是 MMAP 强，还是 FileChannel 强？</p><p>MMAP 众所周知，基于 OS 的 mmap 的内存映射技术，通过 MMU 映射文件，使随机读写文件达到了和读写内存相似的速度。</p><p>那 FileChannel 呢？是零拷贝吗？很遗憾，不是。FileChannel 快，只是因为他是基于 block 的。</p><p>接下来，benchmark everything —— 徐妈.</p><h2 id="Benchmark-？"><a href="#Benchmark-？" class="headerlink" title="Benchmark ？"></a>Benchmark ？</h2><p>如何 Benchmark？ Benchmark 哪些？</p><p>既然是读写文件，自然就要看读写性能，这是最基本的。但，注意，通常 MQ 会使用定时刷盘，防止数据丢失，MMAP 和 FileChannel 都有 force 方法，用于将 pageCache 的数据刷到硬盘上。force 会影响性能吗？ 答案是会。影响到什么程度呢？ 不知道。每次写入的数据大小会影响性能吗，毫无疑问会，但规则是什么呢？FileOutputStream 真的一无是处吗？答案是不一定。</p><p>一直以来，文件调优都是艺术，因为影响性能的因素太多，首先，SSD 的出现，已经让传统基于 B+ tree 的树形结构产生了自我疑问，第二，每个文件系统的性能不同，Linux ext3 和 ext4 性能天壤之别（删除文件的性能差距在 20 倍左右）。而 Max OS 的 HFS+  系统被 Linus 称之为“有史以来最垃圾的文件系统”，幸运的是，苹果终于在 2017 年推送了 macOS High Sierra 和  iOS 10.3  系统，这个两个系统都抛弃了 HFS+，换成了性能更高的 APFS。而每个文件系统又可以设置不同的调度算法，另外，还有虚拟内存缺页中断带来的性能毛刺…….</p><p>（tips：良心的 RocketMQ 提供了 Linux IO 调优的脚本，这点做的不错 ：）</p><p>跑题了。</p><p>楼主写了一个小项目，用于测试 Java MappedByteBuffer &amp; FileChannel &amp; RandomAccessFile  &amp;  FileXXXputStream 的读写性能。大家也可以在自己的机器上跑跑看。</p><h2 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h2><p>CPU：intel i7 4核8线程  4.2GHz</p><p>内存：40GB DDR4</p><p>磁盘：SSD 读写 2GB/s 左右</p><p>JDK1.8</p><p>OS：Mac OS 10.13.6</p><p>虚拟内存： 未关闭，大小 9GB</p><p>测试注意点：</p><ol><li>为了防止 PageCache 缓存的影响，每次都生成一个新的文件进行读取。</li><li>为了测试不同数据包对性能的影响，需要使用不同大小的数据包进行多次测试。</li><li>force 对性能影响很大，应该单独测试。</li><li>使用 1GB 文件进行测试（小文件没有参考意义，大文件 mmap 无法映射）</li></ol><h2 id="纯粹读测试"><a href="#纯粹读测试" class="headerlink" title="纯粹读测试"></a>纯粹读测试</h2><p>1GB 文件：</p><p>测试 MappedByteBuffer &amp; FileChannel &amp; RandomAccessFile &amp; FileInputStream.</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/1557669126293-f1b23e6d-e7a5-49ef-b168-254750b74c7d.png" alt="img"></p><p>从这张图里，我们看到，mmap   性能完胜，特别是在小数据量的情况下。其他的流，只有在4kb 的情况下，才开始反杀 mmap。<strong>因此，读 4kb 以下的数据，请使用 mmap。</strong></p><p>再放大看看 mmap 和 FileChannel 的比较：</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/1557669126451-95cfa344-61b6-4761-85d9-60334d8d9504.png" alt="img"></p><p>根据上图，我们看到，在写入数据包大于 4kb 以上的情况下，FileChannel 等一众非零拷贝，基本完胜 mmap，除了那个一次读 1G 文件的 BT 测试。</p><p><strong>因此，如果你的数据包大于 4kb，请使用 FileChannel</strong>。</p><h2 id="纯粹写测试"><a href="#纯粹写测试" class="headerlink" title="纯粹写测试"></a>纯粹写测试</h2><p>1GB 文件：</p><p>测试 MappedByteBuffer &amp; FileChannel &amp; RandomAccessFile &amp; FileInputStream.</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/1557669126347-04361229-1b8b-4d24-b39f-50bd9cb8ca9d.png" alt="img"></p><p>从上图，我们可以看出，mmap 性能还是一样的稳定。FileChannel 也不差，但是在 32 字节数据量的情况下，还差点意思。</p><p>再看缩略图：</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/1557669126361-7f5ecc22-0dc7-4c69-a466-a165d674d84d.png" alt="img"></p><p>我们看到，64字节 是 FileChannel 和 mmap 性能的分水岭，从  64字节开始，FileChannel 一路反杀，直到 BT 1GB 文件稍稍输了一丢丢。</p><p>因此，我们建议：<strong>如果你的数据包大小在 64 字节以上，请使用 FileChannel 写入。</strong></p><h2 id="异步-force-测试"><a href="#异步-force-测试" class="headerlink" title="异步 force 测试"></a>异步 force 测试</h2><p>我们知道，RocketMQ 使用异步刷盘，那么异步 force 对性能有没有影响呢？benchmark everything。我们使用异步线程，每 16kb 刷盘一次，看看性能如何。</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/1557669126404-c31df221-6555-4f6c-8f12-e66ac645c908.png" alt="img"></p><p>mmap 一直落后，且性能很差，除了在 2048 字节那里有一点点抖动，基本维持 在 4000 左右，而没有 force 的情况下，则在 1500 左右。而 FileChannel 则完全不受 force 的影响。在我的测试中，1GB 的文件，一次 force 需要 800 毫秒左右。buffer 越大，时间越多，反之则越小。</p><p>说个题外话，Kafka 一直不建议使用 force，大概也有这个原因。当然，Kafka 还有自己的多副本策略保证数据安全。</p><p><strong>这里，我们得出结论，如果你需要经常执行 force，即使是异步的，也请一定不要使用 mmap，请使用 FileChannel。</strong></p><h2 id="总结。"><a href="#总结。" class="headerlink" title="总结。"></a>总结。</h2><p>基于以上测试，我们得出一张图表：</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/1557669126639-301ec560-32ee-4d63-817b-739a5fb196bb.png" alt="img"></p><p>现在，假设我们要开发一个文件系统，系统的数据包在 1024 - 2048 byte 左右，我们应该使用什么策略？</p><p>答：读使用 mmap，仅仅写使用 FileChannel。</p><p>再回过头看看 MQ 的实现者们，似乎只有 QMQ 是 这么做的。当然，RocketMQ 也提供了 FileChannel 的选项。但默认  mmap 写加异步刷盘，应该是 broker busy 的元凶吧。</p><p>而 Kafka，因为默认不 force，也是使用 FileChannel 进行写入的，为什么使用 FileChannel 读呢？大概是因为消息的大小在 4kb 以上吧。</p><p>这样一揣测，这些 MQ 的设计似乎都非常合理。</p><p>最后，能不用 force 就别用 force。如果要用 force ，就请使用 FileChannel。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.yuque.com/monaludao/nkut87/vzm91k" target="_blank" rel="noopener">MappedByteBuffer VS FileChannel 孰强孰弱？</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Java 在 JDK 1.4 引入了 ByteBuffer 等 NIO 相关的类，使得 Java 程序员可以抛弃基于 Stream ，从而使
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>17 个方面，综合对比 Kafka、RabbitMQ、RocketMQ、ActiveMQ 四个分布式消息队列</title>
    <link href="http://www.antizhou.com/mq/17%20%E4%B8%AA%E6%96%B9%E9%9D%A2%EF%BC%8C%E7%BB%BC%E5%90%88%E5%AF%B9%E6%AF%94%20Kafka%E3%80%81RabbitMQ%E3%80%81RocketMQ%E3%80%81ActiveMQ%20%E5%9B%9B%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    <id>http://www.antizhou.com/mq/17 个方面，综合对比 Kafka、RabbitMQ、RocketMQ、ActiveMQ 四个分布式消息队列/</id>
    <published>2019-05-13T05:42:15.000Z</published>
    <updated>2019-05-13T05:43:22.272Z</updated>
    
    <content type="html"><![CDATA[<blockquote><ul><li>一、资料文档</li><li>二、开发语言</li><li>三、支持的协议</li><li>四、消息存储</li><li>五、消息事务</li><li>六、负载均衡</li><li>七、集群方式</li><li>八、管理界面</li><li>九、可用性</li><li>十、消息重复</li><li>十一、吞吐量TPS</li><li>十二、订阅形式和消息分发</li><li>十三、顺序消息</li><li>十四、消息确认</li><li>十五、消息回溯</li><li>十六、消息重试</li><li>十七、并发度</li></ul></blockquote><hr><p>本文将从，Kafka、RabbitMQ、ZeroMQ、RocketMQ、ActiveMQ 17 个方面综合对比作为消息队列使用时的差异。</p><h1 id="一、资料文档"><a href="#一、资料文档" class="headerlink" title="一、资料文档"></a>一、资料文档</h1><p>Kafka：中。有kafka作者自己写的书，网上资料也有一些。 rabbitmq：多。有一些不错的书，网上资料多。 zeromq：少。没有专门写zeromq的书，网上的资料多是一些代码的实现和简单介绍。 rocketmq：少。没有专门写rocketmq的书，网上的资料良莠不齐，官方文档很简洁，但是对技术细节没有过多的描述。 activemq：多。没有专门写activemq的书，网上资料多。</p><h1 id="二、开发语言"><a href="#二、开发语言" class="headerlink" title="二、开发语言"></a>二、开发语言</h1><p>Kafka：Scala rabbitmq：Erlang zeromq：c rocketmq：java activemq：java</p><h1 id="三、支持的协议"><a href="#三、支持的协议" class="headerlink" title="三、支持的协议"></a>三、支持的协议</h1><p>Kafka：自己定义的一套…（基于TCP） rabbitmq：AMQP zeromq：TCP、UDP rocketmq：自己定义的一套… activemq：OpenWire、STOMP、REST、XMPP、AMQP</p><h1 id="四、消息存储"><a href="#四、消息存储" class="headerlink" title="四、消息存储"></a>四、消息存储</h1><p>Kafka：内存、磁盘、数据库。支持大量堆积。</p><p>kafka的最小存储单元是分区，一个topic包含多个分区，kafka创建主题时，这些分区会被分配在多个服务器上，通常一个broker一台服务器。 分区首领会均匀地分布在不同的服务器上，分区副本也会均匀的分布在不同的服务器上，确保负载均衡和高可用性，当新的broker加入集群的时候，部分副本会被移动到新的broker上。 根据配置文件中的目录清单，kafka会把新的分区分配给目录清单里分区数最少的目录。 默认情况下，分区器使用轮询算法把消息均衡地分布在同一个主题的不同分区中，对于发送时指定了key的情况，会根据key的hashcode取模后的值存到对应的分区中。</p><p>rabbitmq：内存、磁盘。支持少量堆积。</p><p>rabbitmq的消息分为持久化的消息和非持久化消息，不管是持久化的消息还是非持久化的消息都可以写入到磁盘。 持久化的消息在到达队列时就写入到磁盘，并且如果可以，持久化的消息也会在内存中保存一份备份，这样可以提高一定的性能，当内存吃紧的时候会从内存中清除。非持久化的消息一般只存在于内存中，在内存吃紧的时候会被换入到磁盘中，以节省内存。</p><p>引入镜像队列机制，可将重要队列“复制”到集群中的其他broker上，保证这些队列的消息不会丢失。配置镜像的队列，都包含一个主节点master和多个从节点slave,如果master失效，加入时间最长的slave会被提升为新的master，除发送消息外的所有动作都向master发送，然后由master将命令执行结果广播给各个slave，rabbitmq会让master均匀地分布在不同的服务器上，而同一个队列的slave也会均匀地分布在不同的服务器上，保证负载均衡和高可用性。</p><p>zeromq：消息发送端的内存或者磁盘中。不支持持久化。</p><p>rocketmq：磁盘。支持大量堆积。</p><p>commitLog文件存放实际的消息数据，每个commitLog上限是1G，满了之后会自动新建一个commitLog文件保存数据。ConsumeQueue队列只存放offset、size、tagcode，非常小，分布在多个broker上。ConsumeQueue相当于CommitLog的索引文件，消费者消费时会从consumeQueue中查找消息在commitLog中的offset，再去commitLog中查找元数据。</p><p>ConsumeQueue存储格式的特性，保证了写过程的顺序写盘（写CommitLog文件），大量数据IO都在顺序写同一个commitLog，满1G了再写新的。加上rocketmq是累计4K才强制从PageCache中刷到磁盘（缓存），所以高并发写性能突出。</p><p>activemq：内存、磁盘、数据库。支持少量堆积。</p><h1 id="五、消息事务"><a href="#五、消息事务" class="headerlink" title="五、消息事务"></a>五、消息事务</h1><p>Kafka：支持 rabbitmq：支持。 客户端将信道设置为事务模式，只有当消息被rabbitMq接收，事务才能提交成功，否则在捕获异常后进行回滚。使用事务会使得性能有所下降 zeromq：不支持 rocketmq：支持 activemq：支持</p><h1 id="六、负载均衡"><a href="#六、负载均衡" class="headerlink" title="六、负载均衡"></a>六、负载均衡</h1><p>Kafka：支持负载均衡。</p><p>1&gt;一个broker通常就是一台服务器节点。对于同一个Topic的不同分区，Kafka会尽力将这些分区分布到不同的Broker服务器上，zookeeper保存了broker、主题和分区的元数据信息。分区首领会处理来自客户端的生产请求，kafka分区首领会被分配到不同的broker服务器上，让不同的broker服务器共同分担任务。</p><p>每一个broker都缓存了元数据信息，客户端可以从任意一个broker获取元数据信息并缓存起来，根据元数据信息知道要往哪里发送请求。</p><p>2&gt;kafka的消费者组订阅同一个topic，会尽可能地使得每一个消费者分配到相同数量的分区，分摊负载。</p><p>3&gt;当消费者加入或者退出消费者组的时候，还会触发再均衡，为每一个消费者重新分配分区，分摊负载。</p><p>kafka的负载均衡大部分是自动完成的，分区的创建也是kafka完成的，隐藏了很多细节，避免了繁琐的配置和人为疏忽造成的负载问题。</p><p>4&gt;发送端由topic和key来决定消息发往哪个分区，如果key为null，那么会使用轮询算法将消息均衡地发送到同一个topic的不同分区中。如果key不为null，那么会根据key的hashcode取模计算出要发往的分区。</p><p>rabbitmq：对负载均衡的支持不好。</p><p>1&gt;消息被投递到哪个队列是由交换器和key决定的，交换器、路由键、队列都需要手动创建。</p><p>rabbitmq客户端发送消息要和broker建立连接，需要事先知道broker上有哪些交换器，有哪些队列。通常要声明要发送的目标队列，如果没有目标队列，会在broker上创建一个队列，如果有，就什么都不处理，接着往这个队列发送消息。假设大部分繁重任务的队列都创建在同一个broker上，那么这个broker的负载就会过大。（可以在上线前预先创建队列，无需声明要发送的队列，但是发送时不会尝试创建队列，可能出现找不到队列的问题，rabbitmq的备份交换器会把找不到队列的消息保存到一个专门的队列中，以便以后查询使用）</p><p>使用镜像队列机制建立rabbitmq集群可以解决这个问题，形成master-slave的架构，master节点会均匀分布在不同的服务器上，让每一台服务器分摊负载。slave节点只是负责转发，在master失效时会选择加入时间最长的slave成为master。</p><p>当新节点加入镜像队列的时候，队列中的消息不会同步到新的slave中，除非调用同步命令，但是调用命令后，队列会阻塞，不能在生产环境中调用同步命令。</p><p>2&gt;当rabbitmq队列拥有多个消费者的时候，队列收到的消息将以轮询的分发方式发送给消费者。每条消息只会发送给订阅列表里的一个消费者，不会重复。</p><p>这种方式非常适合扩展，而且是专门为并发程序设计的。</p><p>如果某些消费者的任务比较繁重，那么可以设置basicQos限制信道上消费者能保持的最大未确认消息的数量，在达到上限时，rabbitmq不再向这个消费者发送任何消息。</p><p>3&gt;对于rabbitmq而言，客户端与集群建立的TCP连接不是与集群中所有的节点建立连接，而是挑选其中一个节点建立连接。</p><p>但是rabbitmq集群可以借助HAProxy、LVS技术，或者在客户端使用算法实现负载均衡，引入负载均衡之后，各个客户端的连接可以分摊到集群的各个节点之中。</p><p><strong>客户端均衡算法：</strong></p><p>1)轮询法。按顺序返回下一个服务器的连接地址。</p><p>2)加权轮询法。给配置高、负载低的机器配置更高的权重，让其处理更多的请求；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载。</p><p>3)随机法。随机选取一个服务器的连接地址。</p><p>4)加权随机法。按照概率随机选取连接地址。</p><p>5)源地址哈希法。通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算。</p><p>6)最小连接数法。动态选择当前连接数最少的一台服务器的连接地址。</p><p>zeromq：去中心化，不支持负载均衡。本身只是一个多线程网络库。</p><p>rocketmq：支持负载均衡。</p><p>一个broker通常是一个服务器节点，broker分为master和slave,master和slave存储的数据一样，slave从master同步数据。</p><p>1&gt;nameserver与每个集群成员保持心跳，保存着Topic-Broker路由信息，同一个topic的队列会分布在不同的服务器上。</p><p>2&gt;发送消息通过轮询队列的方式发送，每个队列接收平均的消息量。发送消息指定topic、tags、keys，无法指定投递到哪个队列（没有意义，集群消费和广播消费跟消息存放在哪个队列没有关系）。</p><p>tags选填，类似于 Gmail 为每封邮件设置的标签，方便服务器过滤使用。目前只支 持每个消息设置一个 tag，所以也可以类比为 Notify 的 MessageType 概念。</p><p>keys选填，代表这条消息的业务关键词，服务器会根据 keys 创建哈希索引，设置后， 可以在 Console 系统根据 Topic、Keys 来查询消息，由于是哈希索引，请尽可能 保证 key 唯一，例如订单号，商品 Id 等。</p><p>3&gt;rocketmq的负载均衡策略规定：Consumer数量应该小于等于Queue数量，如果Consumer超过Queue数量，那么多余的Consumer 将不能消费消息。这一点和kafka是一致的，rocketmq会尽可能地为每一个Consumer分配相同数量的队列，分摊负载。</p><p>activemq：支持负载均衡。可以基于zookeeper实现负载均衡。</p><h1 id="七、集群方式"><a href="#七、集群方式" class="headerlink" title="七、集群方式"></a>七、集群方式</h1><p>Kafka：天然的‘Leader-Slave’无状态集群，每台服务器既是Master也是Slave。</p><p>分区首领均匀地分布在不同的kafka服务器上，分区副本也均匀地分布在不同的kafka服务器上，所以每一台kafka服务器既含有分区首领，同时又含有分区副本，每一台kafka服务器是某一台kafka服务器的Slave，同时也是某一台kafka服务器的leader。</p><p>kafka的集群依赖于zookeeper，zookeeper支持热扩展，所有的broker、消费者、分区都可以动态加入移除，而无需关闭服务，与不依靠zookeeper集群的mq相比，这是最大的优势。</p><p>rabbitmq：支持简单集群，’复制’模式，对高级集群模式支持不好。</p><p>rabbitmq的每一个节点，不管是单一节点系统或者是集群中的一部分，要么是内存节点，要么是磁盘节点，集群中至少要有一个是磁盘节点。</p><p>在rabbitmq集群中创建队列，集群只会在单个节点创建队列进程和完整的队列信息（元数据、状态、内容），而不是在所有节点上创建。</p><p>引入镜像队列，可以避免单点故障，确保服务的可用性，但是需要人为地为某些重要的队列配置镜像。</p><p>zeromq：去中心化，不支持集群。</p><p>rocketmq：常用 多对’Master-Slave’ 模式，开源版本需手动切换Slave变成Master</p><p>Name Server是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。</p><p>Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与Name Server集群中的所有节点建立长连接，定时注册Topic信息到所有Name Server。</p><p>Producer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。</p><p>Consumer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。</p><p>客户端先找到NameServer, 然后通过NameServer再找到 Broker。</p><p>一个topic有多个队列，这些队列会均匀地分布在不同的broker服务器上。rocketmq队列的概念和kafka的分区概念是基本一致的，kafka同一个topic的分区尽可能地分布在不同的broker上，分区副本也会分布在不同的broker上。</p><p>rocketmq集群的slave会从master拉取数据备份，master分布在不同的broker上。</p><p>activemq：支持简单集群模式，比如’主-备’，对高级集群模式支持不好。</p><h1 id="八、管理界面"><a href="#八、管理界面" class="headerlink" title="八、管理界面"></a>八、管理界面</h1><p>Kafka：一般 rabbitmq：好 zeromq：无 rocketmq：无 activemq：一般</p><h1 id="九、可用性"><a href="#九、可用性" class="headerlink" title="九、可用性"></a>九、可用性</h1><p>Kafka：非常高（分布式） rabbitmq：高（主从） zeromq：高。 rocketmq：非常高（分布式） activemq：高（主从）</p><h1 id="十、消息重复"><a href="#十、消息重复" class="headerlink" title="十、消息重复"></a>十、消息重复</h1><p>Kafka：支持at least once、at most once</p><p>rabbitmq：支持at least once、at most once</p><p>zeromq：只有重传机制，但是没有持久化，消息丢了重传也没有用。既不是at least once、也不是at most once、更不是exactly only once</p><p>rocketmq：支持at least once</p><p>activemq：支持at least once</p><h1 id="十一、吞吐量TPS"><a href="#十一、吞吐量TPS" class="headerlink" title="十一、吞吐量TPS"></a>十一、吞吐量TPS</h1><p>Kafka：极大 Kafka按批次发送消息和消费消息。发送端将多个小消息合并，批量发向Broker，消费端每次取出一个批次的消息批量处理。 rabbitmq：比较大 zeromq：极大 rocketmq：大 rocketMQ接收端可以批量消费消息，可以配置每次消费的消息数，但是发送端不是批量发送。 activemq：比较大</p><h1 id="十二、订阅形式和消息分发"><a href="#十二、订阅形式和消息分发" class="headerlink" title="十二、订阅形式和消息分发"></a>十二、订阅形式和消息分发</h1><p>Kafka：基于topic以及按照topic进行正则匹配的发布订阅模式。</p><p>【发送】</p><p>发送端由topic和key来决定消息发往哪个分区，如果key为null，那么会使用轮询算法将消息均衡地发送到同一个topic的不同分区中。如果key不为null，那么会根据key的hashcode取模计算出要发往的分区。</p><p>【接收】</p><p>1&gt;consumer向群组协调器broker发送心跳来维持他们和群组的从属关系以及他们对分区的所有权关系，所有权关系一旦被分配就不会改变除非发生再均衡(比如有一个consumer加入或者离开consumer group)，consumer只会从对应的分区读取消息。</p><p>2&gt;kafka限制consumer个数要少于分区个数,每个消息只会被同一个 Consumer Group的一个consumer消费（非广播）。</p><p>3&gt;kafka的 Consumer Group订阅同一个topic，会尽可能地使得每一个consumer分配到相同数量的分区，不同 Consumer Group订阅同一个主题相互独立，同一个消息会被不同的 Consumer Group处理。</p><p>rabbitmq：提供了4种：direct, topic ,Headers和fanout。</p><p>【发送】</p><p>先要声明一个队列，这个队列会被创建或者已经被创建，队列是基本存储单元。</p><p>由exchange和key决定消息存储在哪个队列。</p><p>direct&gt;发送到和bindingKey完全匹配的队列。</p><p>topic&gt;路由key是含有”.”的字符串，会发送到含有“*”、“#”进行模糊匹配的bingKey对应的队列。</p><p>fanout&gt;与key无关，会发送到所有和exchange绑定的队列</p><p>headers&gt;与key无关，消息内容的headers属性（一个键值对）和绑定键值对完全匹配时，会发送到此队列。此方式性能低一般不用</p><p>【接收】</p><p>rabbitmq的队列是基本存储单元，不再被分区或者分片，对于我们已经创建了的队列，消费端要指定从哪一个队列接收消息。</p><p>当rabbitmq队列拥有多个消费者的时候，队列收到的消息将以轮询的分发方式发送给消费者。每条消息只会发送给订阅列表里的一个消费者，不会重复。</p><p>这种方式非常适合扩展，而且是专门为并发程序设计的。</p><p>如果某些消费者的任务比较繁重，那么可以设置basicQos限制信道上消费者能保持的最大未确认消息的数量，在达到上限时，rabbitmq不再向这个消费者发送任何消息。</p><p>zeromq：点对点(p2p)</p><p>rocketmq：基于topic/messageTag以及按照消息类型、属性进行正则匹配的发布订阅模式</p><p>【发送】</p><p>发送消息通过轮询队列的方式发送，每个队列接收平均的消息量。发送消息指定topic、tags、keys，无法指定投递到哪个队列（没有意义，集群消费和广播消费跟消息存放在哪个队列没有关系）。</p><p>tags选填，类似于 Gmail 为每封邮件设置的标签，方便服务器过滤使用。目前只支 持每个消息设置一个 tag，所以也可以类比为 Notify 的 MessageType 概念。</p><p>keys选填，代表这条消息的业务关键词，服务器会根据 keys 创建哈希索引，设置后， 可以在 Console 系统根据 Topic、Keys 来查询消息，由于是哈希索引，请尽可能 保证 key 唯一，例如订单号，商品 Id 等。</p><p>【接收】</p><p>1&gt;广播消费。一条消息被多个Consumer消费，即使Consumer属于同一个ConsumerGroup，消息也会被ConsumerGroup中的每个Consumer都消费一次。</p><p>2&gt;集群消费。一个 Consumer Group中的Consumer实例平均分摊消费消息。例如某个Topic有 9 条消息，其中一个Consumer Group有3个实例，那么每个实例只消费其中的 3 条消息。即每一个队列都把消息轮流分发给每个consumer。</p><p>activemq：点对点(p2p)、广播（发布-订阅）</p><p>点对点模式，每个消息只有1个消费者；</p><p>发布/订阅模式，每个消息可以有多个消费者。</p><p>【发送】</p><p>点对点模式：先要指定一个队列，这个队列会被创建或者已经被创建。</p><p>发布/订阅模式：先要指定一个topic，这个topic会被创建或者已经被创建。</p><p>【接收】</p><p>点对点模式：对于已经创建了的队列，消费端要指定从哪一个队列接收消息。</p><p>发布/订阅模式：对于已经创建了的topic，消费端要指定订阅哪一个topic的消息。</p><h1 id="十三、顺序消息"><a href="#十三、顺序消息" class="headerlink" title="十三、顺序消息"></a>十三、顺序消息</h1><p>Kafka：支持。</p><p>设置生产者的max.in.flight.requests.per.connection为1，可以保证消息是按照发送顺序写入服务器的，即使发生了重试。</p><p>kafka保证同一个分区里的消息是有序的，但是这种有序分两种情况</p><p>1&gt;key为null，消息逐个被写入不同主机的分区中，但是对于每个分区依然是有序的</p><p>2&gt;key不为null , 消息被写入到同一个分区，这个分区的消息都是有序。</p><p>rabbitmq：不支持</p><p>zeromq：不支持</p><p>rocketmq：支持</p><p>activemq：不支持</p><h1 id="十四、消息确认"><a href="#十四、消息确认" class="headerlink" title="十四、消息确认"></a>十四、消息确认</h1><p>Kafka：支持。</p><p>1&gt;发送方确认机制</p><p>ack=0，不管消息是否成功写入分区</p><p>ack=1，消息成功写入首领分区后，返回成功</p><p>ack=all，消息成功写入所有分区后，返回成功。</p><p>2&gt;接收方确认机制</p><p>自动或者手动提交分区偏移量，早期版本的kafka偏移量是提交给Zookeeper的，这样使得zookeeper的压力比较大，更新版本的kafka的偏移量是提交给kafka服务器的，不再依赖于zookeeper群组，集群的性能更加稳定。</p><p>rabbitmq：支持。</p><p>1&gt;发送方确认机制，消息被投递到所有匹配的队列后，返回成功。如果消息和队列是可持久化的，那么在写入磁盘后，返回成功。支持批量确认和异步确认。</p><p>2&gt;接收方确认机制，设置autoAck为false，需要显式确认，设置autoAck为true，自动确认。</p><p>当autoAck为false的时候，rabbitmq队列会分成两部分，一部分是等待投递给consumer的消息，一部分是已经投递但是没收到确认的消息。如果一直没有收到确认信号，并且consumer已经断开连接，rabbitmq会安排这个消息重新进入队列，投递给原来的消费者或者下一个消费者。</p><p>未确认的消息不会有过期时间，如果一直没有确认，并且没有断开连接，rabbitmq会一直等待，rabbitmq允许一条消息处理的时间可以很久很久。</p><p>zeromq：支持。</p><p>rocketmq：支持。</p><p>activemq：支持。</p><h1 id="十五、消息回溯"><a href="#十五、消息回溯" class="headerlink" title="十五、消息回溯"></a>十五、消息回溯</h1><p>Kafka：支持指定分区offset位置的回溯。 rabbitmq：不支持 zeromq：不支持 rocketmq：支持指定时间点的回溯。 activemq：不支持</p><h1 id="十六、消息重试"><a href="#十六、消息重试" class="headerlink" title="十六、消息重试"></a>十六、消息重试</h1><p>Kafka：不支持，但是可以实现。</p><p>kafka支持指定分区offset位置的回溯，可以实现消息重试。</p><p>rabbitmq：不支持，但是可以利用消息确认机制实现。</p><p>rabbitmq接收方确认机制，设置autoAck为false。</p><p>当autoAck为false的时候，rabbitmq队列会分成两部分，一部分是等待投递给consumer的消息，一部分是已经投递但是没收到确认的消息。如果一直没有收到确认信号，并且consumer已经断开连接，rabbitmq会安排这个消息重新进入队列，投递给原来的消费者或者下一个消费者。</p><p>zeromq：不支持，</p><p>rocketmq：支持。</p><p>消息消费失败的大部分场景下，立即重试99%都会失败，所以rocketmq的策略是在消费失败时定时重试，每次时间间隔相同。</p><p>1&gt;发送端的 send 方法本身支持内部重试，重试逻辑如下：</p><p>a)至多重试3次；</p><p>b)如果发送失败，则轮转到下一个broker；</p><p>c)这个方法的总耗时不超过sendMsgTimeout 设置的值，默认 10s，超过时间不在重试。</p><p>2&gt;接收端。</p><p>Consumer 消费消息失败后，要提供一种重试机制，令消息再消费一次。Consumer 消费消息失败通常可以分为以下两种情况：</p><ol><li>由于消息本身的原因，例如反序列化失败，消息数据本身无法处理（例如话费充值，当前消息的手机号被</li></ol><p>注销，无法充值）等。定时重试机制，比如过 10s 秒后再重试。</p><ol><li>由于依赖的下游应用服务不可用，例如 db 连接不可用，外系统网络不可达等。</li></ol><p>即使跳过当前失败的消息，消费其他消息同样也会报错。这种情况可以 sleep 30s，再消费下一条消息，减轻 Broker 重试消息的压力。</p><p>activemq：不支持</p><h1 id="十七、并发度"><a href="#十七、并发度" class="headerlink" title="十七、并发度"></a>十七、并发度</h1><p>Kafka：高</p><p>一个线程一个消费者，kafka限制消费者的个数要小于等于分区数，如果要提高并行度，可以在消费者中再开启多线程，或者增加consumer实例数量。</p><p>rabbitmq：极高</p><p>本身是用Erlang语言写的，并发性能高。</p><p>可在消费者中开启多线程，最常用的做法是一个channel对应一个消费者，每一个线程把持一个channel，多个线程复用connection的tcp连接，减少性能开销。</p><p>当rabbitmq队列拥有多个消费者的时候，队列收到的消息将以轮询的分发方式发送给消费者。每条消息只会发送给订阅列表里的一个消费者，不会重复。</p><p>这种方式非常适合扩展，而且是专门为并发程序设计的。</p><p>如果某些消费者的任务比较繁重，那么可以设置basicQos限制信道上消费者能保持的最大未确认消息的数量，在达到上限时，rabbitmq不再向这个消费者发送任何消息。</p><p>zeromq：高</p><p>rocketmq：高</p><p>1&gt;rocketmq限制消费者的个数少于等于队列数，但是可以在消费者中再开启多线程，这一点和kafka是一致的，提高并行度的方法相同。</p><p>修改消费并行度方法</p><p>a) 同一个 ConsumerGroup 下，通过增加 Consumer 实例数量来提高并行度，超过订阅队列数的 Consumer实例无效。</p><p>b) 提高单个 Consumer 的消费并行线程，通过修改参数consumeThreadMin、consumeThreadMax</p><p>2&gt;同一个网络连接connection，客户端多个线程可以同时发送请求，连接会被复用，减少性能开销。</p><p>activemq：高</p><p>单个ActiveMQ的接收和消费消息的速度在1万笔/秒（持久化 一般为1-2万， 非持久化 2 万以上），在生产环境中部署10个Activemq就能达到10万笔/秒以上的性能，部署越多的activemq broker 在MQ上latency也就越低，系统吞吐量也就越高。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;一、资料文档&lt;/li&gt;
&lt;li&gt;二、开发语言&lt;/li&gt;
&lt;li&gt;三、支持的协议&lt;/li&gt;
&lt;li&gt;四、消息存储&lt;/li&gt;
&lt;li&gt;五、消息事务&lt;/li&gt;
&lt;li&gt;六、负载均衡&lt;/li&gt;
&lt;li&gt;七、集群方式&lt;/li&gt;
&lt;li&gt;八、管理界
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>消息传递模型</title>
    <link href="http://www.antizhou.com/kafka/%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E6%A8%A1%E5%9E%8B/"/>
    <id>http://www.antizhou.com/kafka/消息传递模型/</id>
    <published>2019-05-12T13:46:49.000Z</published>
    <updated>2019-05-12T13:47:27.506Z</updated>
    
    <content type="html"><![CDATA[<p>传统的消息队列最少提供两种消息模型，一种P2P，一种PUB/SUB（发布订阅），而Kafka并没有这么做，巧妙的，它提供了一个消费者组的概念，一个消息可以被多个消费者组消费，但是只能被一个消费者组里的一个消费者消费，这样当只有一个消费者组时就等同与P2P模型，当存在多个消费者组时就是PUB/SUB模型。</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/37237-20160803222049465-823320120.png" alt="img"></p><p>Kafka 的 consumer 是以pull的形式获取消息数据的。 pruducer push消息到kafka cluster ，consumer从集群中pull消息，如下图。该博客主要讲解. Parts在消费者中的分配、以及相关的消费者顺序、底层结构元数据信息、Kafka数据读取和存储等。</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/37237-20160803223835403-497432688.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;传统的消息队列最少提供两种消息模型，一种P2P，一种PUB/SUB（发布订阅），而Kafka并没有这么做，巧妙的，它提供了一个消费者组的概念，一个消息可以被多个消费者组消费，但是只能被一个消费者组里的一个消费者消费，这样当只有一个消费者组时就等同与P2P模型，当存在多个消费
      
    
    </summary>
    
      <category term="kafka" scheme="http://www.antizhou.com/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://www.antizhou.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>为什么Kafka不支持读写分离</title>
    <link href="http://www.antizhou.com/kafka/%E4%B8%BA%E4%BB%80%E4%B9%88Kafka%E4%B8%8D%E6%94%AF%E6%8C%81%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/"/>
    <id>http://www.antizhou.com/kafka/为什么Kafka不支持读写分离/</id>
    <published>2019-05-12T04:51:25.000Z</published>
    <updated>2019-05-12T04:58:00.420Z</updated>
    
    <content type="html"><![CDATA[<p>从代码层面上来说，在 Kafka 中完全可以支持这种功能，但是会大大增加代码的复杂度，所以我们要从“收益点”这个角度来做具体分析。主写从读可以让从节点去分担主节 点的负载压力，预防主节点负载过重而从节点却空闲的情况发生。但是主写从读也有 2 个很明 显的缺点:</p><ul><li>数据一致性问题。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间 窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中 A 数据的值都为 X， 之后将主节点中 A 的值修改为 Y，那么在这个变更通知到从节点之前，应用读取从节点中的 A 数据的值并不为最新的 Y，由此便产生了数据不一致的问题。</li><li>延时问题。类似 Redis 这种组件，数据从写入主节点到同步至从节点中的过程需要经 历网络→主节点内存→网络→从节点内存这几个阶段，整个过程会耗费一定的时间。而在 Kafka 中，主从同步会比 Redis 更加耗时，它需要经历网络→主节点内存→主节点磁盘→网络→从节 点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。</li></ul><p>现实情况下，很多应用既可以忍受一定程度上的延时，也可以忍受一段时间内的数据不一 致的情况，那么对于这种情况，Kafka 是否有必要支持主写从读的功能呢?</p><p>主写从读可以均摊一定的负载却不能做到完全的负载均衡，比如对于数据写压力很大而读 压力很小的情况，从节点只能分摊很少的负载压力，而绝大多数压力还是在主节点上。而在 Kafka 中却可以达到很大程度上的负载均衡，而且这种均衡是在主写主读的架构上实现的。我们来看 一下 Kafka 的生产消费模型，如下图所示。</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/640%3Fwx_fmt%3Djpeg%26wxfrom%3D5%26wx_lazy%3D1%26wx_co%3D1.png" alt="img"></p><p>在 Kafka 集群中有 3 个分区，每个分区有 3 个副本，正好均匀地分布在 3个 broker 上，灰色阴影的代表 leader 副本，非灰色阴影的代表 follower 副本，虚线表示 follower 副本从 leader 副本上拉取消息。当生产者写入消息的时候都写入 leader 副本，对于图 8-23 中的 情形，每个 broker 都有消息从生产者流入;当消费者读取消息的时候也是从 leader 副本中读取 的，对于图 8-23 中的情形，每个 broker 都有消息流出到消费者。</p><p>我们很明显地可以看出，每个 broker 上的读写负载都是一样的，这就说明 Kafka 可以通过 主写主读实现主写从读实现不了的负载均衡。上图展示是一种理想的部署情况，有以下几种 情况(包含但不仅限于)会造成一定程度上的负载不均衡:</p><ul><li>broker 端的分区分配不均。当创建主题的时候可能会出现某些 broker 分配到的分区数 多而其他 broker 分配到的分区数少，那么自然而然地分配到的 leader 副本也就不均。</li><li>生产者写入消息不均。生产者可能只对某些 broker 中的 leader 副本进行大量的写入操 作，而对其他 broker 中的 leader 副本不闻不问。</li><li>消费者消费消息不均。消费者可能只对某些 broker 中的 leader 副本进行大量的拉取操 作，而对其他 broker 中的 leader 副本不闻不问。</li><li>leader 副本的切换不均。在实际应用中可能会由于 broker 宕机而造成主从副本的切换， 或者分区副本的重分配等，这些动作都有可能造成各个 broker 中 leader 副本的分配不均。</li></ul><p>对此，我们可以做一些防范措施。针对第一种情况，在主题创建的时候尽可能使分区分配 得均衡，好在 Kafka 中相应的分配算法也是在极力地追求这一目标，如果是开发人员自定义的 分配，则需要注意这方面的内容。对于第二和第三种情况，主写从读也无法解决。对于第四种 情况，Kafka 提供了优先副本的选举来达到 leader 副本的均衡，与此同时，也可以配合相应的 监控、告警和运维平台来实现均衡的优化。</p><p>所以，从某种意义上来说，主写从读是由于设计上的缺陷而形成的权宜之计。</p><p><a href="https://mp.weixin.qq.com/s/7TdG-y7ZyynRto9nmJMYbw" target="_blank" rel="noopener">阿里面试，为什么Kafka不支持读写分离</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;从代码层面上来说，在 Kafka 中完全可以支持这种功能，但是会大大增加代码的复杂度，所以我们要从“收益点”这个角度来做具体分析。主写从读可以让从节点去分担主节 点的负载压力，预防主节点负载过重而从节点却空闲的情况发生。但是主写从读也有 2 个很明 显的缺点:&lt;/p&gt;
&lt;u
      
    
    </summary>
    
      <category term="kafka" scheme="http://www.antizhou.com/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://www.antizhou.com/tags/kafka/"/>
    
      <category term="读写分离" scheme="http://www.antizhou.com/tags/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/"/>
    
  </entry>
  
  <entry>
    <title>如何保证缓存与数据库的双写一致性</title>
    <link href="http://www.antizhou.com/cache/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E7%BC%93%E5%AD%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    <id>http://www.antizhou.com/cache/如何保证缓存与数据库的双写一致性/</id>
    <published>2019-05-12T04:30:06.000Z</published>
    <updated>2019-05-12T04:32:04.213Z</updated>
    
    <content type="html"><![CDATA[<p>只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？</p><h2 id="面试题剖析"><a href="#面试题剖析" class="headerlink" title="面试题剖析"></a>面试题剖析</h2><p>一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。</p><p>串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上请求。</p><h3 id="Cache-Aside-Pattern"><a href="#Cache-Aside-Pattern" class="headerlink" title="Cache Aside Pattern"></a>Cache Aside Pattern</h3><p>最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。</p><p>读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。</p><p>更新的时候，先更新数据库，然后再删除缓存。</p><p>为什么是删除缓存，而不是更新缓存？</p><p>原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。</p><p>比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。</p><p>另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于比较复杂的缓存数据计算的场景，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，这个缓存到底会不会被频繁访问到？</p><p>举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低，用到缓存才去算缓存。</p><p>其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。</p><h3 id="最初级的缓存不一致问题及解决方案"><a href="#最初级的缓存不一致问题及解决方案" class="headerlink" title="最初级的缓存不一致问题及解决方案"></a>最初级的缓存不一致问题及解决方案</h3><p>问题：先修改数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。</p><p><img src="https://raw.githubusercontent.com/antizhou/material/master/images/640.jpeg" alt="img"></p><p>解决思路：先删除缓存，再修改数据库。如果数据库修改失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中。</p><h3 id="比较复杂的数据不一致问题分析"><a href="#比较复杂的数据不一致问题分析" class="headerlink" title="比较复杂的数据不一致问题分析"></a>比较复杂的数据不一致问题分析</h3><p>数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。</p><p>完了，数据库和缓存中的数据不一样了。。。</p><p>为什么上亿流量高并发场景下，缓存会出现这个问题？</p><p>只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就 1 万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况。</p><p><strong>解决方案如下：</strong></p><p>更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个 jvm 内部队列中。</p><p>一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。</p><p>这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。</p><p>待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。</p><p>如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。</p><p>高并发的场景下，该解决方案要注意的问题：</p><p><strong>1、读请求长时阻塞</strong></p><p>由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。</p><p>该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。</p><p>另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每隔库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 * 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致读请求的长时阻塞。</p><p>一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。</p><p>如果一个内存队列中可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。</p><p>其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。</p><p>实际粗略测算一下</p><p>如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。</p><p>经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。</p><p><strong>2、读请求并发量过高</strong></p><p>这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。</p><p>但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。</p><p><strong>3、多服务实例部署的请求路由</strong></p><p>可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器路由到相同的服务实例上。</p><p>比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。</p><p><strong>4、热点商品的路由问题，导致请求的倾斜</strong></p><p>万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？&lt;/p&gt;
&lt;h2 id=&quot;面试题剖析&quot;&gt;&lt;a href=&quot;#面试题剖析&quot; class=&quot;headerlink&quot; title=&quot;面试题剖析&quot;&gt;&lt;/a&gt;面试题剖
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>netty</title>
    <link href="http://www.antizhou.com/netty/netty/"/>
    <id>http://www.antizhou.com/netty/netty/</id>
    <published>2019-05-05T14:39:00.000Z</published>
    <updated>2019-05-05T14:39:37.482Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://segmentfault.com/a/1190000015895045" target="_blank" rel="noopener">Netty(三) 什么是 TCP 拆、粘包？如何解决？</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://segmentfault.com/a/1190000015895045&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Netty(三) 什么是 TCP 拆、粘包？如何解决？&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>websocket</title>
    <link href="http://www.antizhou.com/websocket/websocket/"/>
    <id>http://www.antizhou.com/websocket/websocket/</id>
    <published>2019-05-04T02:03:51.000Z</published>
    <updated>2019-05-04T02:06:35.591Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ajax轮询"><a href="#ajax轮询" class="headerlink" title="ajax轮询"></a>ajax轮询</h2><p>ajax轮询的原理非常简单，让浏览器隔个几秒就发送一次请求，询问服务器是否有新信息。</p><h2 id="long-poll"><a href="#long-poll" class="headerlink" title="long poll"></a>long poll</h2><p><code>long poll</code> 其实原理跟 <code>ajax轮询</code> 差不多，都是采用轮询的方式，不过采取的是阻塞模型（一直打电话，没收到就不挂电话），也就是说，客户端发起连接后，如果没消息，就一直不返回Response给客户端。直到有消息才返回，返回完之后，客户端再次建立连接，周而复始（等待超时，则重新建立连接）。</p><h2 id="Websocket"><a href="#Websocket" class="headerlink" title="Websocket"></a>Websocket</h2><p>通过上面这个例子，我们可以看出，这两种方式都不是最好的方式，需要很多资源。</p><p><strong>ajax轮询 需要服务器有很快的处理速度和资源。（速度）long poll 需要有很高的并发，也就是说同时接待客户的能力。（场地大小）</strong></p><p>哦对了，忘记说了HTTP还是一个无状态协议。</p><p>通俗的说就是，服务器因为每天要接待太多客户了，是个健忘鬼，你一挂电话，他就把你的东西全忘光了，把你的东西全丢掉了。你第二次还得再告诉服务器一遍。</p><p>所以在这种情况下出现了，Websocket出现了。<strong>他解决了HTTP的这几个难题。首先，被动性，当服务器完成协议升级后（HTTP-&gt;Websocket），服务端就可以主动推送信息给客户端啦。</strong></p><p><strong>其次，这样的协议解决了上面同步有延迟，而且还非常消耗资源的这种情况。</strong>那么为什么他会解决服务器上消耗资源的问题呢？</p><p>其实我们所用的程序是要经过两层代理的，即HTTP协议在Nginx等服务器的解析下，然后再传送给相应的Handler（PHP等）来处理。简单地说，我们有一个非常快速的 <code>接线员（Nginx）</code> ，他负责把问题转交给相应的 <code>客服（Handler）</code> 。</p><p>本身接线员基本上速度是足够的，但是每次都卡在客服（Handler）了，老有客服处理速度太慢。，导致客服不够。Websocket就解决了这样一个难题，建立后，可以直接跟接线员建立持久连接，有信息的时候客服想办法通知接线员，然后接线员在统一转交给客户。</p><p>这样就可以解决客服处理速度过慢的问题了。</p><p>同时，在传统的方式上，要不断的建立，关闭HTTP协议，由于HTTP是非状态性的，每次都要重新传输 <code>identity info</code> （鉴别信息），来告诉服务端你是谁。</p><p>虽然接线员很快速，但是每次都要听这么一堆，效率也会有所下降的，同时还得不断把这些信息转交给客服，不但浪费客服的处理时间，而且还会在网路传输中消耗过多的流量/时间。</p><p>但是Websocket只需要一次HTTP握手，所以说整个通讯过程是建立在一次连接/状态中，也就避免了HTTP的非状态性，服务端会一直知道你的信息，直到你关闭请求，这样就解决了接线员要反复解析HTTP协议，还要查看identity info的信息。</p><p>同时由客户主动询问，转换为服务器（推送）有信息的时候就发送（当然客户端还是等主动发送信息过来的。。），没有信息的时候就交给接线员（Nginx），不需要占用本身速度就慢的客服（Handler）了</p><p><strong>WebSocket协议的目标是在一个独立的持久连接上提供全双工双向通信。客户端和服务器可以向对方主动发送和接受数据。在JS中创建WebSocket后，会有一个HTTP请求发向浏览器以发起请求。在取得服务器响应后，建立的连接会使用HTTP升级将HTTP协议转换为WebSocket协议。也就是说，使用标准的HTTP协议无法实现WebSocket，只有支持那些协议的专门浏览器才能正常工作。</strong></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/fuqiang88/p/5956363.html" target="_blank" rel="noopener">看完让你彻底搞懂Websocket原理</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ajax轮询&quot;&gt;&lt;a href=&quot;#ajax轮询&quot; class=&quot;headerlink&quot; title=&quot;ajax轮询&quot;&gt;&lt;/a&gt;ajax轮询&lt;/h2&gt;&lt;p&gt;ajax轮询的原理非常简单，让浏览器隔个几秒就发送一次请求，询问服务器是否有新信息。&lt;/p&gt;
&lt;h2 id=
      
    
    </summary>
    
      <category term="websocket" scheme="http://www.antizhou.com/categories/websocket/"/>
    
    
      <category term="protocol" scheme="http://www.antizhou.com/tags/protocol/"/>
    
      <category term="websocket" scheme="http://www.antizhou.com/tags/websocket/"/>
    
  </entry>
  
  <entry>
    <title>redis为什么那么快</title>
    <link href="http://www.antizhou.com/redis/redis%E4%B8%BA%E4%BB%80%E4%B9%88%E9%82%A3%E4%B9%88%E5%BF%AB/"/>
    <id>http://www.antizhou.com/redis/redis为什么那么快/</id>
    <published>2019-05-03T09:20:15.000Z</published>
    <updated>2019-05-03T09:21:45.358Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Redis为什么这么快"><a href="#Redis为什么这么快" class="headerlink" title="Redis为什么这么快"></a>Redis为什么这么快</h2><p>1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)；</p><p>2、数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的；</p><p>3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；</p><p>4、使用多路I/O复用模型，非阻塞IO；</p><p>5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；</p><p>以上几点都比较好理解，下边我们针对多路 I/O 复用模型进行简单的探讨：</p><p>1）多路 I/O 复用模型</p><p>多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。</p><p>这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响Redis性能的瓶颈，主要由以上几点造就了 Redis 具有很高的吞吐量。</p><h2 id="那么为什么Redis是单线程的"><a href="#那么为什么Redis是单线程的" class="headerlink" title="那么为什么Redis是单线程的"></a>那么为什么Redis是单线程的</h2><p>我们首先要明白，上边的种种分析，都是为了营造一个Redis很快的氛围！官方FAQ表示，因为Redis是基于内存的操作，<strong>CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽</strong>。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了（毕竟采用多线程会有很多麻烦！）。</p><p>但是，我们使用单线程的方式是无法发挥多核CPU 性能，不过我们可以通过在单机开多个Redis 实例来完善！</p><p>作者：徐刘根<br>来源：CSDN<br>原文：<a href="https://blog.csdn.net/xlgen157387/article/details/79470556" target="_blank" rel="noopener">https://blog.csdn.net/xlgen157387/article/details/79470556</a><br>版权声明：本文为博主原创文章，转载请附上博文链接！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Redis为什么这么快&quot;&gt;&lt;a href=&quot;#Redis为什么这么快&quot; class=&quot;headerlink&quot; title=&quot;Redis为什么这么快&quot;&gt;&lt;/a&gt;Redis为什么这么快&lt;/h2&gt;&lt;p&gt;1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内
      
    
    </summary>
    
      <category term="redis" scheme="http://www.antizhou.com/categories/redis/"/>
    
    
      <category term="redis" scheme="http://www.antizhou.com/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://www.antizhou.com/golang/%E6%9C%AA%E5%91%BD%E5%90%8D/"/>
    <id>http://www.antizhou.com/golang/未命名/</id>
    <published>2018-12-11T12:30:45.144Z</published>
    <updated>2018-12-11T16:44:26.678Z</updated>
    
    <content type="html"><![CDATA[<p>关于 Go 中 Map 类型和 Slice 类型的传递</p><p><strong>Map 类型</strong></p><p>先看例子 m1:</p><p><a href="https://www.jb51.net/article/127552.htm#" target="_blank" rel="noopener">?</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`func main() &#123;``  ``m := ``make``(map[int]int)``  ``mdMap(m)``  ``fmt``.Println(m)``&#125;` `func mdMap(m map[int]int) &#123;``  ``m[1] = 100``  ``m[2] = 200``&#125;`</span><br></pre></td></tr></table></figure><p>结果是</p><p><a href="https://www.jb51.net/article/127552.htm#" target="_blank" rel="noopener">?</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`map[2:200 1:100]`</span><br></pre></td></tr></table></figure><p>我们再修改如下 m2：</p><p><a href="https://www.jb51.net/article/127552.htm#" target="_blank" rel="noopener">?</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`func main() &#123;``  ``var m map[int]int``  ``mdMap(m)``  ``fmt``.Println(m)``&#125;` `func mdMap(m map[int]int) &#123;``  ``m = ``make``(map[int]int)``  ``m[1] = 100``  ``m[2] = 200``&#125;`</span><br></pre></td></tr></table></figure><p>发现结果变成了</p><p><a href="https://www.jb51.net/article/127552.htm#" target="_blank" rel="noopener">?</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`map[]`</span><br></pre></td></tr></table></figure><p>要理解这个问题，需要明确在 Go 中不存在引用传递，所有的参数传递都是值传递。</p><p>现在再来分析下，如图:</p><p><img src="/Users/joyo/develop/git-code/github/homepage/source/_posts/golang/2017115110249824.jpg" alt="img"></p><p>可能有些人会有疑问，为什么途中的 m 像是一个指针呢。查看官方的 Blog 中有写：</p><p><strong>Map types are reference types, like pointers or slices, …</strong></p><p>这边说 Map 类型是引用类型，像是指针或是 Slice（切片）。所以我们基本上可以把它当作是指针来看待（注意，只是近似，或者说其中含有指针，其内部仍然含有其他信息，这里只是为了便于理解），只不过这个指针有些特殊罢了。</p><p>m1 中，当调用 mdMap 方法时重新开辟了内存，将 m 的内容，也就是 map 的地址拷贝入了 m’，所以此时当操作 map 时，m 和 m’ 所指向的内存为同一块，就导致 m 的 map 发生了改变。</p><p>而在 m2 中，在调用 mdMap 之前，m 并未分配内存，也就是说并未指向任何的 map 内存区域。从未导致 m’ 的 map 修改不能反馈到 m 上。</p><p><strong>Slice 类型</strong></p><p>现在看一下 Slice。</p><p>s1:</p><p><a href="https://www.jb51.net/article/127552.htm#" target="_blank" rel="noopener">?</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`func main() &#123;``  ``s := make([]int, 2)``  ``mdSlice(s)``  ``fmt.Println(s)``&#125;` `func mdSlice(s []int) &#123;``  ``s[0] = 1``  ``s[1] = 2``&#125;`</span><br></pre></td></tr></table></figure><p>s2:</p><p><a href="https://www.jb51.net/article/127552.htm#" target="_blank" rel="noopener">?</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`func main() &#123;``  ``var` `s []int``  ``mdSlice(s)``  ``fmt.Println(s)``&#125;` `func mdSlice(s []int) &#123;``  ``s = make([]int, 2)``  ``s[0] = 1``  ``s[1] = 2``&#125;`</span><br></pre></td></tr></table></figure><p>不出所料：</p><p>s1 结果为</p><p>[1 2]</p><p>s2 为</p><p>[]</p><p>因为正如官方所说，Slice 类型与 Map 类型一样，类似于指针，Slice 中仍然含有长度等信息。</p><p>修改一下 s1，变成 s3：</p><p><a href="https://www.jb51.net/article/127552.htm#" target="_blank" rel="noopener">?</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`func main() &#123;``  ``s := make([]int, 2)``  ``mdSlice(s)``  ``fmt.Println(s)``&#125;` `func mdSlice(s []int) &#123;``  ``s = append(s, 1)``  ``s = append(s, 2)``&#125;`</span><br></pre></td></tr></table></figure><p>不再修改 slice 原先的两个元素，而加上另外两个，结果为:</p><p>[0 0]</p><p>发现修改并没有反馈到原先的 slice 上。</p><p>这里我们需要把 slice 想象为特殊的指针，其已经保存了所指向内存区域长度，所以 append 之后的内存并不会反映到 main() 中：</p><p><a href="https://files.jb51.net/file_images/article/201711/2017115110423934.jpg?201710511440" target="_blank" rel="noopener"><img src="/Users/joyo/develop/git-code/github/homepage/source/_posts/golang/2017115110423934.jpg" alt="img"></a></p><p>那如何才能反映到 main() 中呢？没错，使用指向 Slice 的指针。</p><p><a href="https://www.jb51.net/article/127552.htm#" target="_blank" rel="noopener">?</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`func mdSlice(s *[]int) &#123;``  ``*s = append(*s, 1)``  ``*s = append(*s, 2)``&#125;`</span><br></pre></td></tr></table></figure><p>内存如图所示：</p><p><a href="https://files.jb51.net/file_images/article/201711/2017110511003934.png" target="_blank" rel="noopener"><img src="/Users/joyo/develop/git-code/github/homepage/source/_posts/golang/2017110511003934.png" alt="img"></a></p><p>注意本文中内存区域分配是否连续完全随机，不影响程序，只是为了图解清晰。</p><p><strong>Chan 类型</strong></p><p>Go 中 make 函数能创建的数据类型就 3 类：Slice, Map, Chan。不比多说，相比读者已经能想象 Chan 类型的内存模型了。的确如此，读者可以自己尝试，这边就不过多赘述了。（可以通通过 == nil 的比较来进行测试）。</p><p>以上就是本文的全部内容，希望对大家的学习有所帮助，也希望大家多多支持脚本之家。</p><h1 id="2-2-slice"><a href="#2-2-slice" class="headerlink" title="2.2 slice"></a>2.2 slice</h1><p>一个slice是一个数组某个部分的引用。在内存中，它是一个包含3个域的结构体：指向slice中第一个元素的指针，slice的长度，以及slice的容量。长度是下标操作的上界，如x[i]中i必须小于长度。容量是分割操作的上界，如x[i:j]中j不能大于容量。</p><p><img src="/Users/joyo/develop/git-code/github/homepage/source/_posts/golang/godata3.png" alt="img"></p><p>数组的slice并不会实际复制一份数据，它只是创建一个新的数据结构，包含了另外的一个指针，一个长度和一个容量数据。 如同分割一个字符串，分割数组也不涉及复制操作：它只是新建了一个结构来放置一个不同的指针，长度和容量。在例子中，对<code>[]int{2,3,5,7,11}</code>求值操作会创建一个包含五个值的数组，并设置x的属性来描述这个数组。分割表达式<code>x[1:3]</code>并不分配更多的数据：它只是写了一个新的slice结构的属性来引用相同的存储数据。在例子中，长度为2–只有y[0]和y[1]是有效的索引，但是容量为4–y[0:4]是一个有效的分割表达式。</p><p>由于slice是不同于指针的多字长结构，分割操作并不需要分配内存，甚至没有通常被保存在堆中的slice头部。这种表示方法使slice操作和在C中传递指针、长度对一样廉价。Go语言最初使用一个指向以上结构的指针来表示slice，但是这样做意味着每个slice操作都会分配一块新的内存对象。即使使用了快速的分配器，还是给垃圾收集器制造了很多没有必要的工作。移除间接引用及分配操作可以让slice足够廉价，以避免传递显式索引。</p><h2 id="slice的扩容"><a href="#slice的扩容" class="headerlink" title="slice的扩容"></a>slice的扩容</h2><p>其实slice在Go的运行时库中就是一个C语言动态数组的实现，在$GOROOT/src/pkg/runtime/runtime.h中可以看到它的定义：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">struct    Slice</span><br><span class="line">&#123;    // must not move anything</span><br><span class="line">    byte*    array;        // actual data</span><br><span class="line">    uintgo    len;        // number of elements</span><br><span class="line">    uintgo    cap;        // allocated number of elements</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>在对slice进行append等操作时，可能会造成slice的自动扩容。其扩容时的大小增长规则是：</p><ul><li>如果新的大小是当前大小2倍以上，则大小增长为新大小</li><li>否则循环以下操作：如果当前大小小于1024，按每次2倍增长，否则每次按当前大小1/4增长。直到增长的大小超过或等于新大小。</li></ul><h2 id="make和new"><a href="#make和new" class="headerlink" title="make和new"></a>make和new</h2><p>Go有两个数据结构创建函数：new和make。两者的区别在学习Go语言的初期是一个常见的混淆点。基本的区别是<code>new(T)</code>返回一个<code>*T</code>，返回的这个指针可以被隐式地消除引用（图中的黑色箭头）。而<code>make(T, args)</code>返回一个普通的T。通常情况下，T内部有一些隐式的指针（图中的灰色箭头）。一句话，new返回一个指向已清零内存的指针，而make返回一个复杂的结构。</p><p><img src="/Users/joyo/develop/git-code/github/homepage/source/_posts/golang/godata4.png" alt="img"></p><p>有一种方法可以统一这两种创建方式，但是可能会与C/C++的传统有显著不同：定义<code>make(*T)</code>来返回一个指向新分配的T的指针，这样一来，new(Point)得写成make(*Point)。但这样做实在是和人们期望的分配函数太不一样了，所以Go没有采用这种设计。</p><h2 id="slice与unsafe-Pointer相互转换"><a href="#slice与unsafe-Pointer相互转换" class="headerlink" title="slice与unsafe.Pointer相互转换"></a>slice与unsafe.Pointer相互转换</h2><p>有时候可能需要使用一些比较tricky的技巧，比如利用make弄一块内存自己管理，或者用cgo之类的方式得到的内存，转换为Go类型使用。</p><p>从slice中得到一块内存地址是很容易的：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">200</span>)</span><br><span class="line">ptr := unsafe.Pointer(&amp;s[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>从一个内存指针构造出Go语言的slice结构相对麻烦一些，比如其中一种方式：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ptr unsafe.Pointer</span><br><span class="line">s := ((*[<span class="number">1</span>&lt;&lt;<span class="number">10</span>]<span class="keyword">byte</span>)(ptr))[:<span class="number">200</span>]</span><br></pre></td></tr></table></figure><p>先将<code>ptr</code>强制类型转换为另一种指针，一个指向<code>[1&lt;&lt;10]byte</code>数组的指针，这里数组大小其实是假的。然后用slice操作取出这个数组的前200个，于是<code>s</code>就是一个200个元素的slice。</p><p>或者这种方式：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ptr unsafe.Pointer</span><br><span class="line"><span class="keyword">var</span> s1 = <span class="keyword">struct</span> &#123;</span><br><span class="line">    addr <span class="keyword">uintptr</span></span><br><span class="line">    <span class="built_in">len</span> <span class="keyword">int</span></span><br><span class="line">    <span class="built_in">cap</span> <span class="keyword">int</span></span><br><span class="line">&#125;&#123;ptr, length, length&#125;</span><br><span class="line">s := *(*[]<span class="keyword">byte</span>)(unsafe.Pointer(&amp;s1))</span><br></pre></td></tr></table></figure><p>把slice的底层结构写出来，将addr，len，cap等字段写进去，将这个结构体赋给s。相比上一种写法，这种更好的地方在于cap更加自然，虽然上面写法中实际上1&lt;&lt;10就是cap。</p><p>或者使用reflect.SliceHeader的方式来构造slice，比较推荐这种做法：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> o []<span class="keyword">byte</span></span><br><span class="line">sliceHeader := (*reflect.SliceHeader)((unsafe.Pointer(&amp;o)))</span><br><span class="line">sliceHeader.Cap = length</span><br><span class="line">sliceHeader.Len = length</span><br><span class="line">sliceHeader.Data = <span class="keyword">uintptr</span>(ptr)</span><br></pre></td></tr></table></figure><p><a href="https://www.jb51.net/article/127552.htm" target="_blank" rel="noopener">https://www.jb51.net/article/127552.htm</a></p><p><a href="https://halfrost.com/go_slice/" target="_blank" rel="noopener">https://halfrost.com/go_slice/</a></p><p><a href="https://tiancaiamao.gitbooks.io/go-internals/content/zh/02.2.html" target="_blank" rel="noopener">https://tiancaiamao.gitbooks.io/go-internals/content/zh/02.2.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于 Go 中 Map 类型和 Slice 类型的传递&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Map 类型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;先看例子 m1:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.jb51.net/article/127552.htm#&quot; targ
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://www.antizhou.com/%E5%B9%82%E7%AD%89/"/>
    <id>http://www.antizhou.com/幂等/</id>
    <published>2018-12-10T15:41:50.412Z</published>
    <updated>2018-12-11T01:47:45.259Z</updated>
    
    <content type="html"><![CDATA[<p>幂等（idempotent、idempotence）是一个数学与计算机学概念，常见于抽象代数中。</p><p>在编程中一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。幂等函数，或幂等方法，是指可以<strong>使用相同参数重复执行，并能获得相同结果的函数。</strong>这些<a href="https://baike.baidu.com/item/%E5%87%BD%E6%95%B0/301912" target="_blank" rel="noopener">函数</a>不会影响系统状态，也不用担心重复执行会对系统造成改变。例如，“setTrue()”函数就是一个幂等函数,无论多次执行，其结果都是一样的.更复杂的操作幂等保证是<strong>利用唯一交易号(流水号)实现.</strong></p><p>分布式环境中，有些接口是天然保证幂等性的，如查询操作。有些对数据的修改是一个常量，并且无其他记录和操作，那也可以说是具有幂等性的。其他情况下，所有涉及对数据的修改、状态的变更就都有必要防止重复性操作的发生。通过间接的实现接口的幂等性来防止重复操作所带来的影响，成为了一种有效的解决方案。</p><p><a href="https://blog.csdn.net/tomcatAndOracle/article/details/80619255" target="_blank" rel="noopener">https://blog.csdn.net/tomcatAndOracle/article/details/80619255</a></p><h3 id="GTIS"><a href="#GTIS" class="headerlink" title="GTIS"></a>GTIS</h3><p>GTIS就是这样的一个解决方案。它是一个轻量的重复操作关卡系统，它能够确保在分布式环境中操作的唯一性。我们可以用它来间接保证每个操作的幂等性。它具有如下特点：</p><ul><li>高效：低延时，单个方法平均响应时间在2ms内，几乎不会对业务造成影响；</li><li>可靠：提供降级策略，以应对外部存储引擎故障所造成的影响；提供应用鉴权，提供集群配置自定义，降低不同业务之间的干扰；</li><li>简单：接入简捷方便，学习成本低。只需简单的配置，在代码中进行两个方法的调用即可完成所有的接入工作；</li><li>灵活：提供多种接口参数、使用策略，以满足不同的业务需求。</li></ul><h4 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h4><p>基本原理</p><p>GTIS的实现思路是将每一个不同的业务操作赋予其唯一性。这个唯一性是通过对不同操作所对应的唯一的内容特性生成一个唯一的全局ID来实现的。基本原则为：相同的操作生成相同的全局ID；不同的操作生成不同的全局ID。</p><p>生成的全局ID需要存储在外部存储引擎中，数据库、Redis亦或是Tair等均可实现。考虑到Tair天生分布式和持久化的优势，目前的GTIS存储在Tair中。其相应的key和value如下：</p><ul><li>key：将对于不同的业务，采用APP_KEY+业务操作内容特性生成一个唯一标识trans_contents。然后对唯一标识进行加密生成全局ID作为Key。</li><li>value：current_timestamp + trans_contents，current_timestamp用于标识当前的操作线程。</li></ul><p>判断是否重复，主要利用Tair的SETNX方法，如果原来没有值则set且返回成功，如果已经有值则返回失败。</p><p>内部流程</p><p>GTIS的内部实现流程为：</p><ol><li>业务方在业务操作之前，生成一个能够唯一标识该操作的transContents，传入GTIS；</li><li>GTIS根据传入的transContents，用MD5生成全局ID；</li><li>GTIS将全局ID作为key，current_timestamp+transContents作为value放入Tair进行setNx，将结果返回给业务方；</li><li>业务方根据返回结果确定能否开始进行业务操作；</li><li>若能，开始进行操作；若不能，则结束当前操作；</li><li>业务方将操作结果和请求结果传入GTIS，系统进行一次请求结果的检验；</li><li>若该次操作成功，GTIS根据key取出value值，跟传入的返回结果进行比对，如果两者相等，则将该全局ID的过期时间改为较长时间；</li><li>GTIS返回最终结果。</li></ol><p>实现难点</p><p>GTIS的实现难点在于如何保证其判断重复的可靠性。由于分布式环境的复杂度和业务操作的不确定性，在上一章节分布式锁的实现中考虑的网络断开或主机宕机等问题，同样需要在GTIS中设法解决。这里列出几个典型的场景：</p><ul><li>如果操作执行失败，理想的情况应该是另一个相同的操作可以立即进行。因此，需要对业务方的操作结果进行判断，如果操作失败，那么就需要立即删除该全局ID；</li><li>如果操作超时或主机宕机，当前的操作无法告知GTIS操作是否成功。那么我们必须引入超时机制，一旦长时间获取不到业务方的操作反馈，那么也需要该全局ID失效；</li><li>结合上两个场景，既然全局ID会失效并且可能会被删除，那就需要保证删除的不是另一个相同操作的全局ID。这就需要将特殊的标识记录下来，并由此来判断。这里所用的标识为当前时间戳。</li></ul><p>可以看到，解决这些问题的思路，也和上一章节中的实现有很多类似的地方。除此以外，还有更多的场景需要考虑和解决，所有分支流程如下:</p><p><img src="/Users/joyo/develop/git-code/github/homepage/source/_posts/20161008112749906.jpeg" alt="图片描述"></p><h3 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h3><p>使用时，业务方只需要在操作的前后调用GTIS的前置方法和后置方法，如下图所示。如果前置方法返回可进行操作，则说明此时无重复操作，可以进行。否则则直接结束操作。</p><p><img src="/Users/joyo/develop/git-code/github/homepage/source/_posts/20161008112801522.png" alt="图片描述"></p><p>使用方需要考虑的主要是下面两个参数：</p><ul><li>空间全局性：业务方输入的能够标志操作唯一性的内容特性，可以是唯一性的String类型的ID，也可以是map、POJO等形式。如订单ID等</li><li>时间全局性：确定在多长时间内不允许重复，1小时内还是一个月内亦或是永久。</li></ul><p>此外，GTIS还提供了不同的故障处理策略和重试机制，以此来降低外部存储引擎异常对系统造成的影响。</p><p>目前，GTIS已经持续迭代了7个版本，距离第一个版本有近1年之久，先后在美团点评多个项目中稳定运行。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>在分布式环境中，操作互斥性问题和幂等性问题非常普遍。经过分析，我们找出了解决这两个问题的基本思路和实现原理，给出了具体的解决方案。</p><p>针对操作互斥性问题，常见的做法便是通过分布式锁来处理对共享资源的抢占。分布式锁的实现，很大程度借鉴了多线程和多进程环境中的互斥锁的实现原理。只要满足一些存储方面的基本条件，并且能够解决如网络断开等异常情况，那么就可以实现一个分布式锁。目前已经有基于Zookeeper和Redis等存储引擎的比较典型的分布式锁实现。但是由于单存储引擎的局限，我们开发了基于ZooKeeper和Tair的多引擎分布式锁Cerberus，它具有使用灵活方便等诸多优点，还提供了完善的一键降级方案。</p><p>针对操作幂等性问题，我们可以通过防止重复操作来间接的实现接口的幂等性。GTIS提供了一套可靠的解决方法：依赖于存储引擎，通过对不同操作所对应的唯一的内容特性生成一个唯一的全局ID来防止操作重复。</p><p>目前Cerberus分布式锁、GTIS都已应用在生产环境并平稳运行。两者提供的解决方案已经能够解决大多数分布式环境中的操作互斥性和幂等性的问题。值得一提的是，分布式锁和GTIS都不是万能的，它们对外部存储系统的强依赖使得在环境不那么稳定的情况下，对可靠性会造成一定的影响。在并发量过高的情况下，如果不能很好的控制锁的粒度，那么使用分布式锁也是不太合适的。总的来说，分布式环境下的业务场景纷繁复杂，要解决互斥性和幂等性问题还需要结合当前系统架构、业务需求和未来演进综合考虑。Cerberus分布式锁和GTIS也会持续不断地迭代更新，提供更多的引擎选择、更高效可靠的实现方式、更简捷的接入流程，以期满足更复杂的使用场景和业务需求。</p><p><a href="https://blog.csdn.net/tomcatAndOracle/article/details/80619255" target="_blank" rel="noopener">https://blog.csdn.net/tomcatAndOracle/article/details/80619255</a></p><p><a href="https://www.cnblogs.com/weidagang2046/archive/2011/06/04/2063696.html" target="_blank" rel="noopener">https://www.cnblogs.com/weidagang2046/archive/2011/06/04/2063696.html</a></p><p><a href="https://www.cnblogs.com/leechenxiang/p/6626629.html" target="_blank" rel="noopener">https://www.cnblogs.com/leechenxiang/p/6626629.html</a></p><p><a href="http://www.cnblogs.com/hyddd/archive/2009/03/31/1426026.html" target="_blank" rel="noopener">http://www.cnblogs.com/hyddd/archive/2009/03/31/1426026.html</a></p><p><a href="https://hit-alibaba.github.io/interview/basic/network/HTTP.html" target="_blank" rel="noopener">https://hit-alibaba.github.io/interview/basic/network/HTTP.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;幂等（idempotent、idempotence）是一个数学与计算机学概念，常见于抽象代数中。&lt;/p&gt;
&lt;p&gt;在编程中一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。幂等函数，或幂等方法，是指可以&lt;strong&gt;使用相同参数重复执行，并能获得相同结果
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>中证500指数</title>
    <link href="http://www.antizhou.com/invest/%E4%B8%AD%E8%AF%81500%E6%8C%87%E6%95%B0/"/>
    <id>http://www.antizhou.com/invest/中证500指数/</id>
    <published>2018-12-10T05:35:00.000Z</published>
    <updated>2018-12-10T05:35:57.005Z</updated>
    
    <content type="html"><![CDATA[<h2 id="指数简介"><a href="#指数简介" class="headerlink" title="指数简介"></a>指数简介</h2><p>中证500指数由全部A股中剔除沪深300指数成份股及总市值排名前300名的股票后，总市值排名靠前的500只股票组成，综合反映中国A股市场中一批中小市值公司的股票价格表现。</p><h2 id="编制方案"><a href="#编制方案" class="headerlink" title="编制方案"></a>编制方案</h2><h3 id="样板空间"><a href="#样板空间" class="headerlink" title="样板空间"></a>样板空间</h3><p>沪深 300 指数样本空间由同时满足以下条件的沪深 A 股组成：</p><ul><li>非创业板股票：上市时间超过一个季度，除非该股票自上市以来日均 A 股总市值在全部沪深 A 股（非创业板股票）中排在前 30 位；</li><li>非 ST、*ST 股票，非暂停上市股票</li></ul><h3 id="选样方法"><a href="#选样方法" class="headerlink" title="选样方法"></a>选样方法</h3><p>按照以下步骤进行中证 500 指数的样本股选择：</p><ul><li>在样本空间中剔除沪深 300 指数样本股及最近一年日均总市值排名前 300名的股票；</li><li>将剩余股票按照最近一年（新股为上市以来）的最近一年日均成交金额由高到低排名，剔除排名后 20%的股票；</li><li>将剩余股票按照最近一年日均总市值由高到低进行排名，选取排名在前500 名的股票组成中证 500 指数样本股。</li></ul><h2 id="行业权重分布"><a href="#行业权重分布" class="headerlink" title="行业权重分布"></a>行业权重分布</h2><p><img src="/images/中证500行业权重.png" alt="中证500行业权重"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;指数简介&quot;&gt;&lt;a href=&quot;#指数简介&quot; class=&quot;headerlink&quot; title=&quot;指数简介&quot;&gt;&lt;/a&gt;指数简介&lt;/h2&gt;&lt;p&gt;中证500指数由全部A股中剔除沪深300指数成份股及总市值排名前300名的股票后，总市值排名靠前的500只股票组成，综合反映
      
    
    </summary>
    
      <category term="invest" scheme="http://www.antizhou.com/categories/invest/"/>
    
    
      <category term="指数" scheme="http://www.antizhou.com/tags/%E6%8C%87%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>沪深300指数</title>
    <link href="http://www.antizhou.com/invest/%E6%B2%AA%E6%B7%B1300%E6%8C%87%E6%95%B0/"/>
    <id>http://www.antizhou.com/invest/沪深300指数/</id>
    <published>2018-12-10T05:35:00.000Z</published>
    <updated>2018-12-10T05:36:06.906Z</updated>
    
    <content type="html"><![CDATA[<h2 id="指数简介"><a href="#指数简介" class="headerlink" title="指数简介"></a>指数简介</h2><p>沪深300指数由上海和深圳证券市场中市值大、流动性好的300只股票组成，综合反映中国A股市场上市股票价格的整体表现。 “沪深300指数®”商标归属于中证指数有限公司，未经中证指数有限公司事先书面同意，任何人不得以任何形式使用。</p><h2 id="编制方案"><a href="#编制方案" class="headerlink" title="编制方案"></a>编制方案</h2><h3 id="样板空间"><a href="#样板空间" class="headerlink" title="样板空间"></a>样板空间</h3><p>沪深 300 指数样本空间由同时满足以下条件的沪深 A 股组成：</p><ul><li>非创业板股票：上市时间超过一个季度，除非该股票自上市以来日均 A 股总市值在全部沪深 A 股（非创业板股票）中排在前 30 位；创业板股票：上市时间超过三年。</li><li>非 ST、*ST 股票，非暂停上市股票</li></ul><h3 id="选样方法"><a href="#选样方法" class="headerlink" title="选样方法"></a>选样方法</h3><p>沪深 300 指数样本是按照以下方法选择经营状况良好、无违法违规事件、财务报告无重大问题、股票价格无明显异常波动或市场操纵的公司：</p><ul><li>计算样本空间内股票最近一年（新股为上市第四个交易日以来）的 A 股日均成交金额与 A 股日均总市值；</li><li>对样本空间股票在最近一年的 A 股日均成交金额由高到低排名，剔除排名后 50%的股票;</li><li>对剩余股票按照最近一年 A 股日均总市值由高到低排名，选取前 300 名股票作为指数样本。</li></ul><h3 id="指数计算"><a href="#指数计算" class="headerlink" title="指数计算"></a>指数计算</h3><p>沪深 300 指数以“点”为单位，精确到小数点后 3 位。</p><h2 id="行业权重分布"><a href="#行业权重分布" class="headerlink" title="行业权重分布"></a>行业权重分布</h2><p><img src="/images/沪深300行业权重.png" alt="沪深300行业权重"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;指数简介&quot;&gt;&lt;a href=&quot;#指数简介&quot; class=&quot;headerlink&quot; title=&quot;指数简介&quot;&gt;&lt;/a&gt;指数简介&lt;/h2&gt;&lt;p&gt;沪深300指数由上海和深圳证券市场中市值大、流动性好的300只股票组成，综合反映中国A股市场上市股票价格的整体表现。 “沪深
      
    
    </summary>
    
      <category term="invest" scheme="http://www.antizhou.com/categories/invest/"/>
    
    
      <category term="指数" scheme="http://www.antizhou.com/tags/%E6%8C%87%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://www.antizhou.com/invest/%E6%91%98%E8%A6%81/"/>
    <id>http://www.antizhou.com/invest/摘要/</id>
    <published>2018-12-10T04:47:40.305Z</published>
    <updated>2018-12-10T04:47:59.390Z</updated>
    
    <content type="html"><![CDATA[<p>相比市面上把资金面（回复<strong>820</strong>）说成大单流入流出和龙虎榜行情，我更关注真正的大庄大股东的行为。</p><p>相比市面上成天炒各种有的没的，真的假的内幕消息，我只关注公告当中关于并购、定增和激励这些实质性改变公司前景的重大消息。</p><p>随着股市参与者的资金增加，学历提升，盯着一根K线炒股的日子过去了。</p><p>如果自己投资股票，不看公告，我劝你还是退出。</p><p>如果自己投资股票，分不清动态静态LTM市盈率，不知道净利润不正常的时候要看市净率（PB），我劝你也别玩了。</p><p>如果自己投资股票，对宏观政策的理解还是停留在货币战争的阴谋论，我劝你不如买他们站台的P2P。</p><p>如果自己投资股票，自认为是价值投资派，却总想着选中两三支股票，放在那儿两三年，期间什么都不用管，什么都不用愁，回来后还要翻两三倍，我劝你还是换余额宝 - 什么都不用管，什么都不用愁每年的收益就是2-3%。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;相比市面上把资金面（回复&lt;strong&gt;820&lt;/strong&gt;）说成大单流入流出和龙虎榜行情，我更关注真正的大庄大股东的行为。&lt;/p&gt;
&lt;p&gt;相比市面上成天炒各种有的没的，真的假的内幕消息，我只关注公告当中关于并购、定增和激励这些实质性改变公司前景的重大消息。&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>string 数据结构</title>
    <link href="http://www.antizhou.com/golang/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <id>http://www.antizhou.com/golang/数据结构/</id>
    <published>2018-12-09T10:58:09.000Z</published>
    <updated>2018-12-09T11:49:38.010Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/golang/70.png" alt="img"></p><p>字符串默认在堆上分配内存存储。字符串是通过char数组存储的，字符串是不可变的字节数组，其头部指针指向一个字节数组。</p><p>字符串在Go语言内存模型中用一个2字长的数据结构表示。它包含一个指向字符串存储数据的指针和一个长度数据。因为string类型是不可变的，对于多字符串共享同一个存储数据是安全的。切分操作str[i:j]会得到一个新的2字长结构，<strong>一个可能不同的但仍指向同一个字节序列(即上文说的存储数据)的指针和长度数据。</strong>这意味着字符串切分可以在不涉及内存分配或复制操作。这使得字符串切分的效率等同于传递下标。</p><p><strong>字符串默认在堆上分配内存存储</strong>。字符串是通过char数组存储的，字符串是不可变的字节数组，其头部指针指向一个字节数组</p><p>string在内存中的存储结构是长度固定的字节数组，也就是说是字符串是不可变的。当要修改字符串的时候，需要转换为[]byte，修改完成后再转换回来。但是不论怎么转换，都必须重新分配内存，并复制数据，通过加号拼接字符串，<strong>每次都必须重新分配内存。</strong>(可以通过strings.join 和 buffer 来优化)</p><p>string在内存中的存储结构是长度固定的字节数组，也就是说是字符串是不可变的。当要修改字符串的时候，需要转换为[]byte，修改完成后再转换回来。但是不论怎么转换，都必须重新分配内存，并复制数据，通过加号拼接字符串，每次都必须重新分配内存。</p><p>1、type 占一个字节</p><p>2、rune 英文占一个字节，中文占三个字节</p><p>3、string底层是用byte数组存的，并且是不可以改变的。 </p><p>4、在 Go 中，字符串是以 UTF-8 为格式进行存储的，<strong>在字符串上调用 len 函数</strong>，取得的是字符串包含的 byte 的个数</p><p>例如 s:=”你好”  fmt.Println(len(s))  输出结果应该是6，因为中文字符是用3个字节存的。</p><p>所以用string存储unicode的话，如果有中文，按下标是访问不到，如果想要获得我们想要的情况的话，需要先转换为rune切片再使用内置的len函数</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">s := <span class="string">"你好"</span> <span class="comment">// UTF-8</span></span><br><span class="line">fmt.Println(<span class="built_in">len</span>(s))    <span class="comment">//结果：6</span></span><br><span class="line"> </span><br><span class="line">st := []<span class="keyword">rune</span>(s)</span><br><span class="line">fmt.Println(<span class="built_in">len</span>(st))    <span class="comment">//结果：2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/golang/70.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;字符串默认在堆上分配内存存储。字符串是通过char数组存储的，字符串是不可变的字节数组，其头部指针指向一个字节数组。&lt;/p&gt;
&lt;p&gt;字符串在Go语言内存模型中用一个2字长的
      
    
    </summary>
    
    
      <category term="golang" scheme="http://www.antizhou.com/tags/golang/"/>
    
      <category term="数据结构" scheme="http://www.antizhou.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>url &quot;+&quot; 转换</title>
    <link href="http://www.antizhou.com/url%20encode/"/>
    <id>http://www.antizhou.com/url encode/</id>
    <published>2018-12-09T05:30:59.000Z</published>
    <updated>2018-12-09T05:32:50.445Z</updated>
    
    <content type="html"><![CDATA[<p>提供两种解决思路：</p><p>1.前端在传值时，将地址中的参数中含有的加号使用%2B替换掉（一定是大写的B），这样传到java后台时就能正确显示为+号了；</p><p>2.当前端不愿意转换时，后台自行在拦截器或请求URL中对其进行截取，将空格replace成+号，一样可以解决问题。</p><p><a href="https://blog.csdn.net/yy339452689/article/details/80662094" target="_blank" rel="noopener">解决Java获取前端URL中加号（+）被转换成空格的问题</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;提供两种解决思路：&lt;/p&gt;
&lt;p&gt;1.前端在传值时，将地址中的参数中含有的加号使用%2B替换掉（一定是大写的B），这样传到java后台时就能正确显示为+号了；&lt;/p&gt;
&lt;p&gt;2.当前端不愿意转换时，后台自行在拦截器或请求URL中对其进行截取，将空格replace成+号，一样
      
    
    </summary>
    
    
      <category term="url" scheme="http://www.antizhou.com/tags/url/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://www.antizhou.com/%E4%BB%8A%E6%97%A5%E6%91%98%E8%A6%81/"/>
    <id>http://www.antizhou.com/今日摘要/</id>
    <published>2018-12-07T05:31:46.287Z</published>
    <updated>2018-12-10T04:46:26.303Z</updated>
    
    <content type="html"><![CDATA[<p>相比市面上把资金面（回复<strong>820</strong>）说成大单流入流出和龙虎榜行情，我更关注真正的大庄大股东的行为。</p><p>相比市面上成天炒各种有的没的，真的假的内幕消息，我只关注公告当中关于并购、定增和激励这些实质性改变公司前景的重大消息。</p><p>随着股市参与者的资金增加，学历提升，盯着一根K线炒股的日子过去了。</p><p>如果自己投资股票，不看公告，我劝你还是退出。</p><p>如果自己投资股票，分不清动态静态LTM市盈率，不知道净利润不正常的时候要看市净率（PB），我劝你也别玩了。</p><p>如果自己投资股票，对宏观政策的理解还是停留在货币战争的阴谋论，我劝你不如买他们站台的P2P。</p><p>如果自己投资股票，自认为是价值投资派，却总想着选中两三支股票，放在那儿两三年，期间什么都不用管，什么都不用愁，回来后还要翻两三倍，我劝你还是换余额宝 - 什么都不用管，什么都不用愁每年的收益就是2-3%。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;相比市面上把资金面（回复&lt;strong&gt;820&lt;/strong&gt;）说成大单流入流出和龙虎榜行情，我更关注真正的大庄大股东的行为。&lt;/p&gt;
&lt;p&gt;相比市面上成天炒各种有的没的，真的假的内幕消息，我只关注公告当中关于并购、定增和激励这些实质性改变公司前景的重大消息。&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
  </entry>
  
</feed>
